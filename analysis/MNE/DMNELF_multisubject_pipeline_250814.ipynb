{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02e06e9",
   "metadata": {},
   "source": [
    "### Multi-Subject Pipeline for PDA Prediction from EEG Data\n",
    "======================================================\n",
    "\n",
    "This pipeline trains a robust model across multiple subjects to predict\n",
    "Positive Diametric Activity (PDA) between DMN and CEN from EEG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84afbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import stft\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MultiSubjectPDAPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for multi-subject PDA prediction from EEG data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir='./data', output_dir='./results'):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_dir : str\n",
    "            Base directory containing subject data\n",
    "        output_dir : str\n",
    "            Output directory for results\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.subjects_data = {}\n",
    "        self.combined_features = None\n",
    "        self.combined_targets = None\n",
    "        self.subject_ids = None\n",
    "        self.trained_model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Pipeline parameters\n",
    "        self.params = {\n",
    "            'eeg': {\n",
    "                'z_score_threshold': 5.0,\n",
    "                'correlation_threshold': 0.2,\n",
    "                'powerline_threshold': 15.0\n",
    "            },\n",
    "            'pda': {\n",
    "                'hrf_delay': 5.0,\n",
    "                'target_fs': 1.0\n",
    "            },\n",
    "            'stft': {\n",
    "                'window_sec': 1.0,\n",
    "                'overlap': 0.5\n",
    "            },\n",
    "            'lz': {\n",
    "                'window_length': 2.0,\n",
    "                'overlap': 0.5,\n",
    "                'complexity_type': 'exhaustive',\n",
    "                'use_fast': True\n",
    "            },\n",
    "            'features': {\n",
    "                'n_top_features': 20,\n",
    "                'use_lagged_features': True,\n",
    "                'lag_samples': [1, 2, 3, 4, 5, 6]\n",
    "            },\n",
    "            'model': {\n",
    "                'test_size': 0.2,\n",
    "                'cv_folds': 5,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_single_subject(self, subject_id, eeg_file, pda_file):\n",
    "        \"\"\"\n",
    "        Process a single subject's data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_id : str\n",
    "            Subject identifier\n",
    "        eeg_file : str\n",
    "            Path to EEG file\n",
    "        pda_file : str\n",
    "            Path to PDA file (CSV with CEN and DMN columns)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Subject data including features and targets\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing Subject: {subject_id}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        subject_output_dir = os.path.join(self.output_dir, subject_id)\n",
    "        os.makedirs(subject_output_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Load and clean EEG data\n",
    "        print(\"\\n1. Loading and cleaning EEG data...\")\n",
    "        raw_processed, bad_channels, qc_stats = self._clean_eeg_data(\n",
    "            eeg_file, subject_output_dir\n",
    "        )\n",
    "        \n",
    "        # 2. Load PDA data\n",
    "        print(\"\\n2. Loading PDA data...\")\n",
    "        pda_df = pd.read_csv(pda_file)\n",
    "        \n",
    "        # Calculate PDA (CEN - DMN)\n",
    "        if 'cen' in pda_df.columns and 'dmn' in pda_df.columns:\n",
    "            pda_signal = pda_df['cen'].values - pda_df['dmn'].values\n",
    "        else:\n",
    "            raise ValueError(f\"PDA file must contain 'cen' and 'dmn' columns\")\n",
    "        \n",
    "        # 3. Align PDA with EEG\n",
    "        print(\"\\n3. Aligning PDA with EEG...\")\n",
    "        pda_aligned_z, pda_time_aligned = self._align_pda_to_eeg(\n",
    "            pda_signal, raw_processed\n",
    "        )\n",
    "        \n",
    "        # 4. Extract features\n",
    "        print(\"\\n4. Extracting features...\")\n",
    "        \n",
    "        # 4a. STFT features\n",
    "        stft_features = self._extract_stft_features(\n",
    "            raw_processed, pda_aligned_z, pda_time_aligned\n",
    "        )\n",
    "        \n",
    "        # 4b. LZ complexity features\n",
    "        lz_features = self._extract_lz_features(\n",
    "            raw_processed, pda_aligned_z, pda_time_aligned\n",
    "        )\n",
    "        \n",
    "        # 4c. Advanced features (spatial, connectivity, etc.)\n",
    "        advanced_features = self._extract_advanced_features(\n",
    "            raw_processed, stft_features, pda_aligned_z, pda_time_aligned\n",
    "        )\n",
    "        \n",
    "        # 5. Combine all features\n",
    "        print(\"\\n5. Combining features...\")\n",
    "        all_features = self._combine_features(\n",
    "            stft_features, lz_features, advanced_features\n",
    "        )\n",
    "        \n",
    "        # 6. Save subject data\n",
    "        subject_data = {\n",
    "            'subject_id': subject_id,\n",
    "            'features': all_features['feature_matrix'],\n",
    "            'feature_names': all_features['feature_names'],\n",
    "            'target': pda_aligned_z,\n",
    "            'time_points': pda_time_aligned,\n",
    "            'eeg_info': {\n",
    "                'n_channels': len(raw_processed.ch_names),\n",
    "                'bad_channels': bad_channels,\n",
    "                'duration': raw_processed.times[-1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        np.savez(\n",
    "            os.path.join(subject_output_dir, f'{subject_id}_processed.npz'),\n",
    "            **subject_data\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nSubject {subject_id} processing complete!\")\n",
    "        print(f\"Features shape: {subject_data['features'].shape}\")\n",
    "        print(f\"Target shape: {subject_data['target'].shape}\")\n",
    "        \n",
    "        return subject_data\n",
    "    \n",
    "    def _clean_eeg_data(self, eeg_file, output_dir):\n",
    "        \"\"\"Clean EEG data using automated bad channel detection.\"\"\"\n",
    "        from automated_channel_cleaning import automated_channel_cleaning\n",
    "        \n",
    "        # Load raw data\n",
    "        raw = mne.io.read_raw_edf(eeg_file, preload=True)\n",
    "        \n",
    "        # Run automated cleaning\n",
    "        raw_cleaned, bad_channels, qc_stats = automated_channel_cleaning(\n",
    "            raw,\n",
    "            z_score_threshold=self.params['eeg']['z_score_threshold'],\n",
    "            correlation_threshold=self.params['eeg']['correlation_threshold'],\n",
    "            powerline_threshold=self.params['eeg']['powerline_threshold'],\n",
    "            save_report=True,\n",
    "            report_path=os.path.join(output_dir, 'qc_report.png')\n",
    "        )\n",
    "        \n",
    "        # Remove bad channels\n",
    "        good_channels = [ch for ch in raw_cleaned.ch_names if ch not in bad_channels]\n",
    "        raw_processed = raw_cleaned.copy().pick_channels(good_channels)\n",
    "        \n",
    "        return raw_processed, bad_channels, qc_stats\n",
    "    \n",
    "    def _align_pda_to_eeg(self, pda_signal, raw_data):\n",
    "        \"\"\"Align PDA signal to EEG timeline with HRF delay.\"\"\"\n",
    "        # Original PDA timing\n",
    "        pda_fs = 1 / 1.2  # ~0.833 Hz\n",
    "        pda_time_orig = np.arange(len(pda_signal)) * 1.2\n",
    "        \n",
    "        # Apply HRF shift\n",
    "        hrf_delay = self.params['pda']['hrf_delay']\n",
    "        pda_time_shifted = pda_time_orig - hrf_delay\n",
    "        \n",
    "        # Target time points\n",
    "        eeg_duration = raw_data.times[-1]\n",
    "        target_fs = self.params['pda']['target_fs']\n",
    "        target_time = np.arange(0, min(eeg_duration, pda_time_shifted[-1]), 1/target_fs)\n",
    "        \n",
    "        # Only use valid PDA samples\n",
    "        valid_idx = pda_time_shifted >= 0\n",
    "        \n",
    "        # Interpolate\n",
    "        interp_func = interp1d(\n",
    "            pda_time_shifted[valid_idx], \n",
    "            pda_signal[valid_idx], \n",
    "            kind='cubic',\n",
    "            bounds_error=False,\n",
    "            fill_value='extrapolate'\n",
    "        )\n",
    "        pda_resampled = interp_func(target_time)\n",
    "        \n",
    "        # Z-score normalization\n",
    "        pda_resampled_z = (pda_resampled - np.mean(pda_resampled)) / np.std(pda_resampled)\n",
    "        \n",
    "        return pda_resampled_z, target_time\n",
    "    \n",
    "    def _extract_stft_features(self, raw_processed, pda_aligned_z, pda_time_aligned):\n",
    "        \"\"\"Extract STFT-based band power features.\"\"\"\n",
    "        fs = raw_processed.info['sfreq']\n",
    "        \n",
    "        # STFT parameters\n",
    "        window_sec = self.params['stft']['window_sec']\n",
    "        nperseg = int(window_sec * fs)\n",
    "        noverlap = int(self.params['stft']['overlap'] * fs)\n",
    "        \n",
    "        # Get EEG data\n",
    "        picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "        eeg_data = raw_processed.get_data(picks=picks_eeg)\n",
    "        \n",
    "        # Compute STFT\n",
    "        f, t_stft, Zxx = stft(eeg_data, fs=fs, nperseg=nperseg, \n",
    "                             noverlap=noverlap, axis=1)\n",
    "        \n",
    "        # Calculate power\n",
    "        power = np.abs(Zxx) ** 2\n",
    "        power = power.transpose(2, 0, 1)  # (n_windows, n_channels, n_freqs)\n",
    "        \n",
    "        # Extract band powers\n",
    "        bands = {\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30),\n",
    "            'low_gamma': (30, 50)\n",
    "        }\n",
    "        \n",
    "        band_powers = {}\n",
    "        for band_name, (low, high) in bands.items():\n",
    "            band_mask = (f >= low) & (f <= high)\n",
    "            band_powers[band_name] = np.mean(power[:, :, band_mask], axis=2)\n",
    "        \n",
    "        # Add ratio features\n",
    "        eps = 1e-10\n",
    "        band_powers['theta_alpha_ratio'] = band_powers['theta'] / (band_powers['alpha'] + eps)\n",
    "        band_powers['beta_alpha_ratio'] = band_powers['beta'] / (band_powers['alpha'] + eps)\n",
    "        \n",
    "        return {\n",
    "            'band_powers': band_powers,\n",
    "            'stft_times': t_stft,\n",
    "            'power': power,\n",
    "            'freqs': f\n",
    "        }\n",
    "    \n",
    "    def _extract_lz_features(self, raw_processed, pda_aligned_z, pda_time_aligned):\n",
    "        \"\"\"Extract Lempel-Ziv complexity features.\"\"\"\n",
    "        from lz_complexity import compute_lz_features_matlab_style\n",
    "        \n",
    "        picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "        eeg_data = raw_processed.get_data(picks=picks_eeg)\n",
    "        fs = raw_processed.info['sfreq']\n",
    "        \n",
    "        # Compute LZ complexity\n",
    "        features = compute_lz_features_matlab_style(\n",
    "            eeg_data, fs,\n",
    "            window_length=self.params['lz']['window_length'],\n",
    "            overlap=self.params['lz']['overlap'],\n",
    "            complexity_type=self.params['lz']['complexity_type'],\n",
    "            use_fast=self.params['lz']['use_fast']\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_advanced_features(self, raw_processed, stft_features, \n",
    "                                  pda_aligned_z, pda_time_aligned):\n",
    "        \"\"\"Extract advanced features including spatial and connectivity.\"\"\"\n",
    "        fs = raw_processed.info['sfreq']\n",
    "        picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "        channel_names = [raw_processed.ch_names[i] for i in picks_eeg]\n",
    "        \n",
    "        band_powers = stft_features['band_powers']\n",
    "        stft_times = stft_features['stft_times']\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Spatial features\n",
    "        frontal_channels = []\n",
    "        parietal_channels = []\n",
    "        \n",
    "        for i, ch in enumerate(channel_names):\n",
    "            ch_upper = ch.upper()\n",
    "            if any(x in ch_upper for x in ['FP1', 'FP2', 'F3', 'F4', 'F7', 'F8', 'FZ']):\n",
    "                frontal_channels.append(i)\n",
    "            if any(x in ch_upper for x in ['P3', 'P4', 'PZ', 'P7', 'P8']):\n",
    "                parietal_channels.append(i)\n",
    "        \n",
    "        # Frontal-Parietal alpha gradient\n",
    "        if frontal_channels and parietal_channels:\n",
    "            frontal_alpha = np.mean(band_powers['alpha'][:, frontal_channels], axis=1)\n",
    "            parietal_alpha = np.mean(band_powers['alpha'][:, parietal_channels], axis=1)\n",
    "            features['frontal_parietal_alpha_gradient'] = parietal_alpha - frontal_alpha\n",
    "        \n",
    "        # Network indices\n",
    "        if frontal_channels and parietal_channels:\n",
    "            dmn_index = (np.mean(band_powers['alpha'][:, parietal_channels], axis=1) - \n",
    "                        np.mean(band_powers['beta'][:, frontal_channels], axis=1))\n",
    "            cen_index = (np.mean(band_powers['beta'][:, frontal_channels], axis=1) - \n",
    "                        np.mean(band_powers['alpha'][:, parietal_channels], axis=1))\n",
    "        else:\n",
    "            dmn_index = np.mean(band_powers['alpha'], axis=1) - np.mean(band_powers['beta'], axis=1)\n",
    "            cen_index = np.mean(band_powers['beta'], axis=1) - np.mean(band_powers['alpha'], axis=1)\n",
    "        \n",
    "        features['dmn_index'] = dmn_index\n",
    "        features['cen_index'] = cen_index\n",
    "        features['network_competition'] = cen_index - dmn_index\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'times': stft_times\n",
    "        }\n",
    "    \n",
    "    def _combine_features(self, stft_features, lz_features, advanced_features):\n",
    "        \"\"\"Combine all features and align with PDA.\"\"\"\n",
    "        from scipy.interpolate import interp1d\n",
    "        \n",
    "        all_aligned_features = {}\n",
    "        \n",
    "        # Add STFT band powers\n",
    "        for band_name, band_data in stft_features['band_powers'].items():\n",
    "            # Mean across channels\n",
    "            feat_mean = np.mean(band_data, axis=1)\n",
    "            all_aligned_features[f'{band_name}_mean'] = feat_mean\n",
    "            \n",
    "            # Std across channels\n",
    "            feat_std = np.std(band_data, axis=1)\n",
    "            all_aligned_features[f'{band_name}_std'] = feat_std\n",
    "        \n",
    "        # Add LZ features\n",
    "        for feat_name, feat_data in lz_features.items():\n",
    "            if feat_name != 'lz_times' and isinstance(feat_data, np.ndarray):\n",
    "                all_aligned_features[feat_name] = feat_data\n",
    "        \n",
    "        # Add advanced features\n",
    "        for feat_name, feat_data in advanced_features['features'].items():\n",
    "            if isinstance(feat_data, np.ndarray):\n",
    "                all_aligned_features[feat_name] = feat_data\n",
    "        \n",
    "        # Get common time array (use STFT times as reference)\n",
    "        ref_times = stft_features['stft_times']\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_list = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for feat_name, feat_data in all_aligned_features.items():\n",
    "            if feat_data.ndim == 1 and len(feat_data) == len(ref_times):\n",
    "                feature_list.append(feat_data.reshape(-1, 1))\n",
    "                feature_names.append(feat_name)\n",
    "        \n",
    "        feature_matrix = np.hstack(feature_list)\n",
    "        \n",
    "        # Add lagged features if requested\n",
    "        if self.params['features']['use_lagged_features']:\n",
    "            lag_samples = self.params['features']['lag_samples']\n",
    "            key_features = ['network_competition', 'cen_index', 'dmn_index', \n",
    "                          'beta_alpha_ratio_mean', 'exhaustive_lz_complexity_mean']\n",
    "            \n",
    "            for lag in lag_samples:\n",
    "                for feat in key_features:\n",
    "                    if feat in feature_names:\n",
    "                        feat_idx = feature_names.index(feat)\n",
    "                        lagged_feat = np.roll(feature_matrix[:, feat_idx], lag)\n",
    "                        lagged_feat[:lag] = lagged_feat[lag]\n",
    "                        \n",
    "                        feature_list.append(lagged_feat.reshape(-1, 1))\n",
    "                        feature_names.append(f'{feat}_lag{lag}s')\n",
    "            \n",
    "            feature_matrix = np.hstack(feature_list)\n",
    "        \n",
    "        return {\n",
    "            'feature_matrix': feature_matrix,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "    \n",
    "    def process_all_subjects(self, subject_files):\n",
    "        \"\"\"\n",
    "        Process all subjects in the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_files : list of tuples\n",
    "            List of (subject_id, eeg_file, pda_file) tuples\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PROCESSING {len(subject_files)} SUBJECTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for subject_id, eeg_file, pda_file in subject_files:\n",
    "            try:\n",
    "                subject_data = self.process_single_subject(\n",
    "                    subject_id, eeg_file, pda_file\n",
    "                )\n",
    "                self.subjects_data[subject_id] = subject_data\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing subject {subject_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n\\nSuccessfully processed {len(self.subjects_data)} subjects\")\n",
    "    \n",
    "    def combine_subjects_data(self):\n",
    "        \"\"\"Combine data from all subjects for training.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"COMBINING MULTI-SUBJECT DATA\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_targets = []\n",
    "        all_subject_ids = []\n",
    "        \n",
    "        # Get common feature names (use first subject as reference)\n",
    "        ref_subject = list(self.subjects_data.values())[0]\n",
    "        common_features = ref_subject['feature_names']\n",
    "        \n",
    "        for subject_id, data in self.subjects_data.items():\n",
    "            # Ensure feature consistency\n",
    "            if data['feature_names'] == common_features:\n",
    "                all_features.append(data['features'])\n",
    "                all_targets.append(data['target'])\n",
    "                all_subject_ids.extend([subject_id] * len(data['target']))\n",
    "            else:\n",
    "                print(f\"Warning: Feature mismatch for subject {subject_id}\")\n",
    "        \n",
    "        # Combine arrays\n",
    "        self.combined_features = np.vstack(all_features)\n",
    "        self.combined_targets = np.hstack(all_targets)\n",
    "        self.subject_ids = np.array(all_subject_ids)\n",
    "        self.feature_names = common_features\n",
    "        \n",
    "        print(f\"\\nCombined data shape:\")\n",
    "        print(f\"  Features: {self.combined_features.shape}\")\n",
    "        print(f\"  Targets: {self.combined_targets.shape}\")\n",
    "        print(f\"  Subjects: {len(np.unique(self.subject_ids))}\")\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"Train and evaluate multiple models with cross-validation.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"TRAINING MULTI-SUBJECT MODELS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Feature selection\n",
    "        top_features, correlations = self._select_top_features()\n",
    "        \n",
    "        # Create feature matrix with selected features\n",
    "        feature_indices = [self.feature_names.index(f) for f in top_features]\n",
    "        X = self.combined_features[:, feature_indices]\n",
    "        y = self.combined_targets\n",
    "        groups = self.subject_ids\n",
    "        \n",
    "        # Standardize features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Models to evaluate\n",
    "        models = {\n",
    "            'Ridge': Ridge(alpha=1.0),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=2000),\n",
    "            'RandomForest': RandomForestRegressor(\n",
    "                n_estimators=100, max_depth=5, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'XGBoost': xgb.XGBRegressor(\n",
    "                n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                random_state=42, n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Group K-Fold for subject-wise cross-validation\n",
    "        gkf = GroupKFold(n_splits=self.params['model']['cv_folds'])\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(\"\\nCross-Validation Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # Cross-validation with groups\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in gkf.split(X_scaled, y, groups):\n",
    "                X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                \n",
    "                corr, _ = pearsonr(y_val, y_pred)\n",
    "                cv_scores.append(corr)\n",
    "            \n",
    "            cv_scores = np.array(cv_scores)\n",
    "            \n",
    "            # Fit on full data\n",
    "            model.fit(X_scaled, y)\n",
    "            y_pred_full = model.predict(X_scaled)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'cv_scores': cv_scores,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'full_correlation': pearsonr(y, y_pred_full)[0],\n",
    "                'full_r2': r2_score(y, y_pred_full)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  CV correlations: {cv_scores.round(3)}\")\n",
    "            print(f\"  Mean CV r: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
    "            print(f\"  Full data r: {results[model_name]['full_correlation']:.3f}\")\n",
    "        \n",
    "        # Select best model\n",
    "        best_model_name = max(results.keys(), \n",
    "                            key=lambda x: results[x]['cv_mean'])\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"BEST MODEL: {best_model_name}\")\n",
    "        print(f\"Mean CV correlation: {results[best_model_name]['cv_mean']:.3f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Store results\n",
    "        self.model_results = results\n",
    "        self.best_model = results[best_model_name]['model']\n",
    "        self.selected_features = top_features\n",
    "        \n",
    "        # Create visualization\n",
    "        self._plot_model_results(results, X_scaled, y, groups)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _select_top_features(self):\n",
    "        \"\"\"Select top features based on correlation with target.\"\"\"\n",
    "        correlations = {}\n",
    "        \n",
    "        for i, feat_name in enumerate(self.feature_names):\n",
    "            feat_data = self.combined_features[:, i]\n",
    "            if np.std(feat_data) > 1e-10:\n",
    "                corr, _ = pearsonr(feat_data, self.combined_targets)\n",
    "                correlations[feat_name] = abs(corr)\n",
    "        \n",
    "        # Sort by correlation\n",
    "        sorted_features = sorted(correlations.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top N features\n",
    "        n_features = self.params['features']['n_top_features']\n",
    "        top_features = [f[0] for f in sorted_features[:n_features]]\n",
    "        \n",
    "        print(f\"\\nTop {n_features} features:\")\n",
    "        for i, (feat, corr) in enumerate(sorted_features[:n_features]):\n",
    "            print(f\"{i+1:2d}. {feat:<40} |r|={corr:.3f}\")\n",
    "        \n",
    "        return top_features, correlations\n",
    "    \n",
    "    def _plot_model_results(self, results, X_scaled, y, groups):\n",
    "        \"\"\"Create visualization of model results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Multi-Subject Model Results', fontsize=16)\n",
    "        \n",
    "        # 1. Model comparison\n",
    "        ax = axes[0, 0]\n",
    "        model_names = list(results.keys())\n",
    "        cv_means = [results[m]['cv_mean'] for m in model_names]\n",
    "        cv_stds = [results[m]['cv_std'] for m in model_names]\n",
    "        \n",
    "        bars = ax.bar(model_names, cv_means, yerr=cv_stds, capsize=5)\n",
    "        ax.set_ylabel('Mean CV Correlation')\n",
    "        ax.set_title('Model Performance Comparison')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Highlight best model\n",
    "        best_idx = np.argmax(cv_means)\n",
    "        bars[best_idx].set_color('darkgreen')\n",
    "        \n",
    "        # 2. Subject-wise performance\n",
    "        ax = axes[0, 1]\n",
    "        best_model = self.best_model\n",
    "        \n",
    "        subject_correlations = {}\n",
    "        unique_subjects = np.unique(groups)\n",
    "        \n",
    "        for subject in unique_subjects:\n",
    "            subject_mask = groups == subject\n",
    "            X_subj = X_scaled[subject_mask]\n",
    "            y_subj = y[subject_mask]\n",
    "            \n",
    "            y_pred = best_model.predict(X_subj)\n",
    "            corr, _ = pearsonr(y_subj, y_pred)\n",
    "            subject_correlations[subject] = corr\n",
    "        \n",
    "        subjects = list(subject_correlations.keys())\n",
    "        correlations = list(subject_correlations.values())\n",
    "        \n",
    "        ax.bar(range(len(subjects)), correlations)\n",
    "        ax.set_xlabel('Subject')\n",
    "        ax.set_ylabel('Correlation')\n",
    "        ax.set_title('Per-Subject Performance (Best Model)')\n",
    "        ax.set_xticks(range(len(subjects)))\n",
    "        ax.set_xticklabels(subjects, rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 3. Feature importance (for tree-based models)\n",
    "        ax = axes[1, 0]\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            importances = self.best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            \n",
    "            top_features = [self.selected_features[i] for i in indices]\n",
    "            top_importances = importances[indices]\n",
    "            \n",
    "            y_pos = np.arange(len(top_features))\n",
    "            ax.barh(y_pos, top_importances)\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(top_features)\n",
    "            ax.set_xlabel('Importance')\n",
    "            ax.set_title('Top 10 Feature Importances')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model',\n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # 4. Predictions scatter plot\n",
    "        ax = axes[1, 1]\n",
    "        y_pred = self.best_model.predict(X_scaled)\n",
    "        \n",
    "        # Downsample for clarity\n",
    "        downsample = 10\n",
    "        ax.scatter(y[::downsample], y_pred[::downsample], \n",
    "                  alpha=0.5, s=20)\n",
    "        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual PDA')\n",
    "        ax.set_ylabel('Predicted PDA')\n",
    "        ax.set_title(f'Predictions (r={pearsonr(y, y_pred)[0]:.3f})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'model_results.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def test_on_new_subject(self, subject_id, eeg_file, pda_file=None):\n",
    "        \"\"\"\n",
    "        Test the trained model on a new subject.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_id : str\n",
    "            Subject identifier\n",
    "        eeg_file : str\n",
    "            Path to EEG file\n",
    "        pda_file : str, optional\n",
    "            Path to PDA file (for evaluation)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Test results including predictions\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TESTING ON NEW SUBJECT: {subject_id}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Process the subject\n",
    "        test_data = self.process_single_subject(subject_id, eeg_file, pda_file)\n",
    "        \n",
    "        # Extract selected features\n",
    "        feature_indices = [test_data['feature_names'].index(f) \n",
    "                          for f in self.selected_features]\n",
    "        X_test = test_data['features'][:, feature_indices]\n",
    "        \n",
    "        # Scale features\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.best_model.predict(X_test_scaled)\n",
    "        \n",
    "        results = {\n",
    "            'subject_id': subject_id,\n",
    "            'predictions': predictions,\n",
    "            'time_points': test_data['time_points']\n",
    "        }\n",
    "        \n",
    "        # If true PDA provided, evaluate\n",
    "        if pda_file is not None:\n",
    "            y_true = test_data['target']\n",
    "            correlation, _ = pearsonr(y_true, predictions)\n",
    "            r2 = r2_score(y_true, predictions)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "            \n",
    "            results.update({\n",
    "                'true_pda': y_true,\n",
    "                'correlation': correlation,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nTest Results:\")\n",
    "            print(f\"  Correlation: {correlation:.3f}\")\n",
    "            print(f\"  R²: {r2:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            \n",
    "            # Plot results\n",
    "            self._plot_test_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_test_results(self, results):\n",
    "        \"\"\"Plot test results for a single subject.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        time_points = results['time_points']\n",
    "        predictions = results['predictions']\n",
    "        true_pda = results['true_pda']\n",
    "        \n",
    "        # Time series\n",
    "        ax = axes[0]\n",
    "        ax.plot(time_points, true_pda, 'k-', label='True PDA', alpha=0.8)\n",
    "        ax.plot(time_points, predictions, 'r--', label='Predicted PDA', alpha=0.8)\n",
    "        ax.set_ylabel('PDA (z-score)')\n",
    "        ax.set_title(f\"Test Subject: {results['subject_id']} (r={results['correlation']:.3f})\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        ax = axes[1]\n",
    "        residuals = true_pda - predictions\n",
    "        ax.plot(time_points, residuals, 'g-', alpha=0.7)\n",
    "        ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        ax.set_title('Prediction Errors')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f\"test_{results['subject_id']}.png\"),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, filename='multi_subject_pda_model.pkl'):\n",
    "        \"\"\"Save the trained model and necessary components.\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.best_model,\n",
    "            'scaler': self.scaler,\n",
    "            'selected_features': self.selected_features,\n",
    "            'model_results': self.model_results,\n",
    "            'params': self.params,\n",
    "            'n_subjects_trained': len(np.unique(self.subject_ids))\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"\\nModel saved to: {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously trained model.\"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.best_model = model_data['model']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.selected_features = model_data['selected_features']\n",
    "        self.model_results = model_data['model_results']\n",
    "        self.params = model_data['params']\n",
    "        print(f\"Model loaded from: {filepath}\")\n",
    "        print(f\"Trained on {model_data['n_subjects_trained']} subjects\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f358e4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROCESSING 5 SUBJECTS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf001\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf001: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf004\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf004: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf005\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf005: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf006\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf006: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf007\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf007: No module named 'automated_channel_cleaning'\n",
      "\n",
      "\n",
      "Successfully processed 0 subjects\n",
      "\n",
      "======================================================================\n",
      "COMBINING MULTI-SUBJECT DATA\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mprocess_all_subjects(training_subjects)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Combine data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mcombine_subjects_data()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m     26\u001b[0m results \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtrain_models()\n",
      "Cell \u001b[0;32mIn[1], line 450\u001b[0m, in \u001b[0;36mMultiSubjectPDAPipeline.combine_subjects_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m all_subject_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Get common feature names (use first subject as reference)\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m ref_subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects_data\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    451\u001b[0m common_features \u001b[38;5;241m=\u001b[39m ref_subject[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject_id, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# Ensure feature consistency\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = MultiSubjectPDAPipeline(\n",
    "        base_dir='./data',\n",
    "        output_dir='./results_multi_subject'\n",
    "    )\n",
    "    \n",
    "    # Example: Process training subjects\n",
    "    training_subjects = [\n",
    "        ('sub-dmnelf001', '../edfs/sub-dmnelf001_task-feedback-run01.edf', './task_output/sub-dmnelf001_DMN_Feedback_run01_roi_outputs.csv'),\n",
    "        ('sub-dmnelf004', './edfs/sub02_eeg.edf', './data/sub02_pda.csv'),\n",
    "        ('sub-dmnelf005', './edfs/sub03_eeg.edf', './data/sub03_pda.csv'),\n",
    "        ('sub-dmnelf006', './edfs/sub03_eeg.edf', './data/sub03_pda.csv'),\n",
    "        ('sub-dmnelf007', './edfs/sub03_eeg.edf', './data/sub03_pda.csv'),\n",
    "        # Add more subjects...\n",
    "    ]\n",
    "    \n",
    "    # Process all subjects\n",
    "    pipeline.process_all_subjects(training_subjects)\n",
    "    \n",
    "    # Combine data\n",
    "    pipeline.combine_subjects_data()\n",
    "    \n",
    "    # Train models\n",
    "    results = pipeline.train_models()\n",
    "    \n",
    "    # Save model\n",
    "    pipeline.save_model()\n",
    "    \n",
    "    # Test on new subject\n",
    "    test_results = pipeline.test_on_new_subject(\n",
    "        'sub_test',\n",
    "        './data/sub_test_eeg.edf',\n",
    "        './data/sub_test_pda.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33dd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
