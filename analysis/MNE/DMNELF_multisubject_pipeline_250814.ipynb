{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Subject Pipeline for PDA Prediction from EEG Data\n",
    "======================================================\n",
    "\n",
    "This pipeline trains a robust model across multiple subjects to predict\n",
    "Positive Diametric Activity (PDA) between DMN and CEN from EEG features."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:analysis/MNE/DMNELF_multisubject_pipeline_250814.ipynb
   "execution_count": 1,
   "id": "f84afbda",
=======
   "execution_count": 5,
>>>>>>> f2d28158e59dec2435ac8d2d3ddc6555b838b99c:analysis/MNE/DMNELF_multisubject_pipeline_250813.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Multi-Subject PDA Pipeline files...\n",
      "==================================================\n",
      "Created: multi_subject_pipeline.py\n",
      "Created: automated_channel_cleaning.py\n",
      "Created: lz_complexity.py\n",
      "Created: run_pipeline.py\n",
      "\n",
      "==================================================\n",
      "Setup complete!\n",
      "==================================================\n",
      "\n",
      "Created files:\n",
      "  - multi_subject_pipeline.py (main pipeline)\n",
      "  - automated_channel_cleaning.py (EEG cleaning)\n",
      "  - lz_complexity.py (LZ features)\n",
      "  - run_pipeline.py (ready to run)\n",
      "\n",
      "To start processing your data, run:\n",
      "  python run_pipeline.py\n",
      "\n",
      "Note: Make sure you have all required packages installed:\n",
      "  pip install mne numpy pandas scipy scikit-learn xgboost tqdm matplotlib seaborn joblib\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup Script - Creates all necessary pipeline files\n",
    "==================================================\n",
    "\n",
    "Run this script first to create all the required Python modules.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "def create_file(filename, content):\n",
    "    \"\"\"Create a file with the given content.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"Created: {filename}\")\n",
    "\n",
    "def setup_pipeline_files():\n",
    "    \"\"\"Create all necessary pipeline files.\"\"\"\n",
    "    \n",
    "    print(\"Setting up Multi-Subject PDA Pipeline files...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Create multi_subject_pipeline.py\n",
    "    multi_subject_pipeline_content = '''\"\"\"\n",
    "Multi-Subject Pipeline for PDA Prediction from EEG Data\n",
    "======================================================\n",
    "\n",
    "This pipeline trains a robust model across multiple subjects to predict\n",
    "Positive Diametric Activity (PDA) between DMN and CEN from EEG features.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import stft\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MultiSubjectPDAPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for multi-subject PDA prediction from EEG data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir='./data', output_dir='./results'):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_dir : str\n",
    "            Base directory containing subject data\n",
    "        output_dir : str\n",
    "            Output directory for results\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.subjects_data = {}\n",
    "        self.combined_features = None\n",
    "        self.combined_targets = None\n",
    "        self.subject_ids = None\n",
    "        self.trained_model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Pipeline parameters\n",
    "        self.params = {\n",
    "            'eeg': {\n",
    "                'z_score_threshold': 5.0,\n",
    "                'correlation_threshold': 0.2,\n",
    "                'powerline_threshold': 15.0\n",
    "            },\n",
    "            'pda': {\n",
    "                'hrf_delay': 5.0,\n",
    "                'target_fs': 1.0\n",
    "            },\n",
    "            'stft': {\n",
    "                'window_sec': 1.0,\n",
    "                'overlap': 0.5\n",
    "            },\n",
    "            'lz': {\n",
    "                'window_length': 2.0,\n",
    "                'overlap': 0.5,\n",
    "                'complexity_type': 'exhaustive',\n",
    "                'use_fast': True\n",
    "            },\n",
    "            'features': {\n",
    "                'n_top_features': 20,\n",
    "                'use_lagged_features': True,\n",
    "                'lag_samples': [1, 2, 3, 4, 5, 6]\n",
    "            },\n",
    "            'model': {\n",
    "                'test_size': 0.2,\n",
    "                'cv_folds': 5,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_single_subject(self, subject_id, eeg_file, pda_file):\n",
    "        \"\"\"\n",
    "        Process a single subject's data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_id : str\n",
    "            Subject identifier\n",
    "        eeg_file : str\n",
    "            Path to EEG file\n",
    "        pda_file : str\n",
    "            Path to PDA file (CSV with CEN and DMN columns)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Subject data including features and targets\n",
    "        \"\"\"\n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(f\"Processing Subject: {subject_id}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        subject_output_dir = os.path.join(self.output_dir, subject_id)\n",
    "        os.makedirs(subject_output_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Load and clean EEG data\n",
    "        print(\"\\\\n1. Loading and cleaning EEG data...\")\n",
    "        raw_processed, bad_channels, qc_stats = self._clean_eeg_data(\n",
    "            eeg_file, subject_output_dir\n",
    "        )\n",
    "        \n",
    "        # 2. Load PDA data\n",
    "        print(\"\\\\n2. Loading PDA data...\")\n",
    "        pda_df = pd.read_csv(pda_file)\n",
    "        \n",
    "        # Calculate PDA (CEN - DMN)\n",
    "        if 'cen' in pda_df.columns and 'dmn' in pda_df.columns:\n",
    "            pda_signal = pda_df['cen'].values - pda_df['dmn'].values\n",
    "        else:\n",
    "            raise ValueError(f\"PDA file must contain 'cen' and 'dmn' columns\")\n",
    "        \n",
    "        # 3. Align PDA with EEG\n",
    "        print(\"\\\\n3. Aligning PDA with EEG...\")\n",
    "        pda_aligned_z, pda_time_aligned = self._align_pda_to_eeg(\n",
    "            pda_signal, raw_processed\n",
    "        )\n",
    "        \n",
    "        # 4. Extract features\n",
    "        print(\"\\\\n4. Extracting features...\")\n",
    "        \n",
    "        # 4a. STFT features\n",
    "        stft_features = self._extract_stft_features(\n",
    "            raw_processed, pda_aligned_z, pda_time_aligned\n",
    "        )\n",
    "        \n",
    "        # 4b. LZ complexity features\n",
    "        lz_features = self._extract_lz_features(\n",
    "            raw_processed, pda_aligned_z, pda_time_aligned\n",
    "        )\n",
    "        \n",
    "        # 4c. Advanced features (spatial, connectivity, etc.)\n",
    "        advanced_features = self._extract_advanced_features(\n",
    "            raw_processed, stft_features, pda_aligned_z, pda_time_aligned\n",
    "        )\n",
    "        \n",
    "        # 5. Combine all features\n",
    "        print(\"\\\\n5. Combining features...\")\n",
    "        all_features = self._combine_features(\n",
    "            stft_features, lz_features, advanced_features\n",
    "        )\n",
    "        \n",
    "        # 6. Save subject data\n",
    "        subject_data = {\n",
    "            'subject_id': subject_id,\n",
    "            'features': all_features['feature_matrix'],\n",
    "            'feature_names': all_features['feature_names'],\n",
    "            'target': pda_aligned_z,\n",
    "            'time_points': pda_time_aligned,\n",
    "            'eeg_info': {\n",
    "                'n_channels': len(raw_processed.ch_names),\n",
    "                'bad_channels': bad_channels,\n",
    "                'duration': raw_processed.times[-1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        np.savez(\n",
    "            os.path.join(subject_output_dir, f'{subject_id}_processed.npz'),\n",
    "            **subject_data\n",
    "        )\n",
    "        \n",
    "        print(f\"\\\\nSubject {subject_id} processing complete!\")\n",
    "        print(f\"Features shape: {subject_data['features'].shape}\")\n",
    "        print(f\"Target shape: {subject_data['target'].shape}\")\n",
    "        \n",
    "        return subject_data\n",
    "    \n",
    "    def _clean_eeg_data(self, eeg_file, output_dir):\n",
    "        \"\"\"Clean EEG data using automated bad channel detection.\"\"\"\n",
    "        from automated_channel_cleaning import automated_channel_cleaning\n",
    "        \n",
    "        # Load raw data\n",
    "        raw = mne.io.read_raw_edf(eeg_file, preload=True)\n",
    "        \n",
    "        # Run automated cleaning\n",
    "        raw_cleaned, bad_channels, qc_stats = automated_channel_cleaning(\n",
    "            raw,\n",
    "            z_score_threshold=self.params['eeg']['z_score_threshold'],\n",
    "            correlation_threshold=self.params['eeg']['correlation_threshold'],\n",
    "            powerline_threshold=self.params['eeg']['powerline_threshold'],\n",
    "            save_report=True,\n",
    "            report_path=os.path.join(output_dir, 'qc_report.png')\n",
    "        )\n",
    "        \n",
    "        # Remove bad channels\n",
    "        good_channels = [ch for ch in raw_cleaned.ch_names if ch not in bad_channels]\n",
    "        raw_processed = raw_cleaned.copy().pick_channels(good_channels)\n",
    "        \n",
    "        return raw_processed, bad_channels, qc_stats\n",
    "    \n",
    "    def _align_pda_to_eeg(self, pda_signal, raw_data):\n",
    "        \"\"\"Align PDA signal to EEG timeline with HRF delay.\"\"\"\n",
    "        # Original PDA timing\n",
    "        pda_fs = 1 / 1.2  # ~0.833 Hz\n",
    "        pda_time_orig = np.arange(len(pda_signal)) * 1.2\n",
    "        \n",
    "        # Apply HRF shift\n",
    "        hrf_delay = self.params['pda']['hrf_delay']\n",
    "        pda_time_shifted = pda_time_orig - hrf_delay\n",
    "        \n",
    "        # Target time points\n",
    "        eeg_duration = raw_data.times[-1]\n",
    "        target_fs = self.params['pda']['target_fs']\n",
    "        target_time = np.arange(0, min(eeg_duration, pda_time_shifted[-1]), 1/target_fs)\n",
    "        \n",
    "        # Only use valid PDA samples\n",
    "        valid_idx = pda_time_shifted >= 0\n",
    "        \n",
    "        # Interpolate\n",
    "        interp_func = interp1d(\n",
    "            pda_time_shifted[valid_idx], \n",
    "            pda_signal[valid_idx], \n",
    "            kind='cubic',\n",
    "            bounds_error=False,\n",
    "            fill_value='extrapolate'\n",
    "        )\n",
    "        pda_resampled = interp_func(target_time)\n",
    "        \n",
    "        # Z-score normalization\n",
    "        pda_resampled_z = (pda_resampled - np.mean(pda_resampled)) / np.std(pda_resampled)\n",
    "        \n",
    "        return pda_resampled_z, target_time\n",
    "    \n",
    "    def _extract_stft_features(self, raw_processed, pda_aligned_z, pda_time_aligned):\n",
    "        \"\"\"Extract STFT-based band power features.\"\"\"\n",
    "        fs = raw_processed.info['sfreq']\n",
    "        \n",
    "        # STFT parameters\n",
    "        window_sec = self.params['stft']['window_sec']\n",
    "        nperseg = int(window_sec * fs)\n",
    "        noverlap = int(self.params['stft']['overlap'] * fs)\n",
    "        \n",
    "        # Get EEG data\n",
    "        picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "        eeg_data = raw_processed.get_data(picks=picks_eeg)\n",
    "        \n",
    "        # Compute STFT\n",
    "        f, t_stft, Zxx = stft(eeg_data, fs=fs, nperseg=nperseg, \n",
    "                             noverlap=noverlap, axis=1)\n",
    "        \n",
    "        # Calculate power\n",
    "        power = np.abs(Zxx) ** 2\n",
    "        power = power.transpose(2, 0, 1)  # (n_windows, n_channels, n_freqs)\n",
    "        \n",
    "        # Extract band powers\n",
    "        bands = {\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30),\n",
    "            'low_gamma': (30, 50)\n",
    "        }\n",
    "        \n",
    "        band_powers = {}\n",
    "        for band_name, (low, high) in bands.items():\n",
    "            band_mask = (f >= low) & (f <= high)\n",
    "            band_powers[band_name] = np.mean(power[:, :, band_mask], axis=2)\n",
    "        \n",
    "        # Add ratio features\n",
    "        eps = 1e-10\n",
    "        band_powers['theta_alpha_ratio'] = band_powers['theta'] / (band_powers['alpha'] + eps)\n",
    "        band_powers['beta_alpha_ratio'] = band_powers['beta'] / (band_powers['alpha'] + eps)\n",
    "        \n",
    "        return {\n",
    "            'band_powers': band_powers,\n",
    "            'stft_times': t_stft,\n",
    "            'power': power,\n",
    "            'freqs': f\n",
    "        }\n",
    "    \n",
    "    def _extract_lz_features(self, raw_processed, pda_aligned_z, pda_time_aligned):\n",
    "        \"\"\"Extract Lempel-Ziv complexity features.\"\"\"\n",
    "        from lz_complexity import compute_lz_features_matlab_style\n",
    "        \n",
    "        picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "        eeg_data = raw_processed.get_data(picks=picks_eeg)\n",
    "        fs = raw_processed.info['sfreq']\n",
    "        \n",
    "        # Compute LZ complexity\n",
    "        features = compute_lz_features_matlab_style(\n",
    "            eeg_data, fs,\n",
    "            window_length=self.params['lz']['window_length'],\n",
    "            overlap=self.params['lz']['overlap'],\n",
    "            complexity_type=self.params['lz']['complexity_type'],\n",
    "            use_fast=self.params['lz']['use_fast']\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_advanced_features(self, raw_processed, stft_features, \n",
    "                                  pda_aligned_z, pda_time_aligned):\n",
    "        \"\"\"Extract advanced features including spatial and connectivity.\"\"\"\n",
    "        fs = raw_processed.info['sfreq']\n",
    "        picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "        channel_names = [raw_processed.ch_names[i] for i in picks_eeg]\n",
    "        \n",
    "        band_powers = stft_features['band_powers']\n",
    "        stft_times = stft_features['stft_times']\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Spatial features\n",
    "        frontal_channels = []\n",
    "        parietal_channels = []\n",
    "        \n",
    "        for i, ch in enumerate(channel_names):\n",
    "            ch_upper = ch.upper()\n",
    "            if any(x in ch_upper for x in ['FP1', 'FP2', 'F3', 'F4', 'F7', 'F8', 'FZ']):\n",
    "                frontal_channels.append(i)\n",
    "            if any(x in ch_upper for x in ['P3', 'P4', 'PZ', 'P7', 'P8']):\n",
    "                parietal_channels.append(i)\n",
    "        \n",
    "        # Frontal-Parietal alpha gradient\n",
    "        if frontal_channels and parietal_channels:\n",
    "            frontal_alpha = np.mean(band_powers['alpha'][:, frontal_channels], axis=1)\n",
    "            parietal_alpha = np.mean(band_powers['alpha'][:, parietal_channels], axis=1)\n",
    "            features['frontal_parietal_alpha_gradient'] = parietal_alpha - frontal_alpha\n",
    "        \n",
    "        # Network indices\n",
    "        if frontal_channels and parietal_channels:\n",
    "            dmn_index = (np.mean(band_powers['alpha'][:, parietal_channels], axis=1) - \n",
    "                        np.mean(band_powers['beta'][:, frontal_channels], axis=1))\n",
    "            cen_index = (np.mean(band_powers['beta'][:, frontal_channels], axis=1) - \n",
    "                        np.mean(band_powers['alpha'][:, parietal_channels], axis=1))\n",
    "        else:\n",
    "            dmn_index = np.mean(band_powers['alpha'], axis=1) - np.mean(band_powers['beta'], axis=1)\n",
    "            cen_index = np.mean(band_powers['beta'], axis=1) - np.mean(band_powers['alpha'], axis=1)\n",
    "        \n",
    "        features['dmn_index'] = dmn_index\n",
    "        features['cen_index'] = cen_index\n",
    "        features['network_competition'] = cen_index - dmn_index\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'times': stft_times\n",
    "        }\n",
    "    \n",
    "    def _combine_features(self, stft_features, lz_features, advanced_features):\n",
    "        \"\"\"Combine all features and align with PDA.\"\"\"\n",
    "        from scipy.interpolate import interp1d\n",
    "        \n",
    "        all_aligned_features = {}\n",
    "        \n",
    "        # Add STFT band powers\n",
    "        for band_name, band_data in stft_features['band_powers'].items():\n",
    "            # Mean across channels\n",
    "            feat_mean = np.mean(band_data, axis=1)\n",
    "            all_aligned_features[f'{band_name}_mean'] = feat_mean\n",
    "            \n",
    "            # Std across channels\n",
    "            feat_std = np.std(band_data, axis=1)\n",
    "            all_aligned_features[f'{band_name}_std'] = feat_std\n",
    "        \n",
    "        # Add LZ features\n",
    "        for feat_name, feat_data in lz_features.items():\n",
    "            if feat_name != 'lz_times' and isinstance(feat_data, np.ndarray):\n",
    "                all_aligned_features[feat_name] = feat_data\n",
    "        \n",
    "        # Add advanced features\n",
    "        for feat_name, feat_data in advanced_features['features'].items():\n",
    "            if isinstance(feat_data, np.ndarray):\n",
    "                all_aligned_features[feat_name] = feat_data\n",
    "        \n",
    "        # Get common time array (use STFT times as reference)\n",
    "        ref_times = stft_features['stft_times']\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_list = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for feat_name, feat_data in all_aligned_features.items():\n",
    "            if feat_data.ndim == 1 and len(feat_data) == len(ref_times):\n",
    "                feature_list.append(feat_data.reshape(-1, 1))\n",
    "                feature_names.append(feat_name)\n",
    "        \n",
    "        feature_matrix = np.hstack(feature_list)\n",
    "        \n",
    "        # Add lagged features if requested\n",
    "        if self.params['features']['use_lagged_features']:\n",
    "            lag_samples = self.params['features']['lag_samples']\n",
    "            key_features = ['network_competition', 'cen_index', 'dmn_index', \n",
    "                          'beta_alpha_ratio_mean', 'lz_complexity_mean']\n",
    "            \n",
    "            for lag in lag_samples:\n",
    "                for feat in key_features:\n",
    "                    if feat in feature_names:\n",
    "                        feat_idx = feature_names.index(feat)\n",
    "                        lagged_feat = np.roll(feature_matrix[:, feat_idx], lag)\n",
    "                        lagged_feat[:lag] = lagged_feat[lag]\n",
    "                        \n",
    "                        feature_list.append(lagged_feat.reshape(-1, 1))\n",
    "                        feature_names.append(f'{feat}_lag{lag}s')\n",
    "            \n",
    "            feature_matrix = np.hstack(feature_list)\n",
    "        \n",
    "        return {\n",
    "            'feature_matrix': feature_matrix,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "    \n",
    "    def process_all_subjects(self, subject_files):\n",
    "        \"\"\"\n",
    "        Process all subjects in the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_files : list of tuples\n",
    "            List of (subject_id, eeg_file, pda_file) tuples\n",
    "        \"\"\"\n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(f\"PROCESSING {len(subject_files)} SUBJECTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for subject_id, eeg_file, pda_file in subject_files:\n",
    "            try:\n",
    "                subject_data = self.process_single_subject(\n",
    "                    subject_id, eeg_file, pda_file\n",
    "                )\n",
    "                self.subjects_data[subject_id] = subject_data\n",
    "            except Exception as e:\n",
    "                print(f\"\\\\nError processing subject {subject_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\\\n\\\\nSuccessfully processed {len(self.subjects_data)} subjects\")\n",
    "    \n",
    "    def combine_subjects_data(self):\n",
    "        \"\"\"Combine data from all subjects for training.\"\"\"\n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(\"COMBINING MULTI-SUBJECT DATA\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_targets = []\n",
    "        all_subject_ids = []\n",
    "        \n",
    "        # Get common feature names (use first subject as reference)\n",
    "        ref_subject = list(self.subjects_data.values())[0]\n",
    "        common_features = ref_subject['feature_names']\n",
    "        \n",
    "        for subject_id, data in self.subjects_data.items():\n",
    "            # Ensure feature consistency\n",
    "            if data['feature_names'] == common_features:\n",
    "                all_features.append(data['features'])\n",
    "                all_targets.append(data['target'])\n",
    "                all_subject_ids.extend([subject_id] * len(data['target']))\n",
    "            else:\n",
    "                print(f\"Warning: Feature mismatch for subject {subject_id}\")\n",
    "        \n",
    "        # Combine arrays\n",
    "        self.combined_features = np.vstack(all_features)\n",
    "        self.combined_targets = np.hstack(all_targets)\n",
    "        self.subject_ids = np.array(all_subject_ids)\n",
    "        self.feature_names = common_features\n",
    "        \n",
    "        print(f\"\\\\nCombined data shape:\")\n",
    "        print(f\"  Features: {self.combined_features.shape}\")\n",
    "        print(f\"  Targets: {self.combined_targets.shape}\")\n",
    "        print(f\"  Subjects: {len(np.unique(self.subject_ids))}\")\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"Train and evaluate multiple models with cross-validation.\"\"\"\n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(\"TRAINING MULTI-SUBJECT MODELS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Feature selection\n",
    "        top_features, correlations = self._select_top_features()\n",
    "        \n",
    "        # Create feature matrix with selected features\n",
    "        feature_indices = [self.feature_names.index(f) for f in top_features]\n",
    "        X = self.combined_features[:, feature_indices]\n",
    "        y = self.combined_targets\n",
    "        groups = self.subject_ids\n",
    "        \n",
    "        # Standardize features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Models to evaluate\n",
    "        models = {\n",
    "            'Ridge': Ridge(alpha=1.0),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=2000),\n",
    "            'RandomForest': RandomForestRegressor(\n",
    "                n_estimators=100, max_depth=5, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'XGBoost': xgb.XGBRegressor(\n",
    "                n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                random_state=42, n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Group K-Fold for subject-wise cross-validation\n",
    "        gkf = GroupKFold(n_splits=self.params['model']['cv_folds'])\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(\"\\\\nCross-Validation Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # Cross-validation with groups\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in gkf.split(X_scaled, y, groups):\n",
    "                X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                \n",
    "                corr, _ = pearsonr(y_val, y_pred)\n",
    "                cv_scores.append(corr)\n",
    "            \n",
    "            cv_scores = np.array(cv_scores)\n",
    "            \n",
    "            # Fit on full data\n",
    "            model.fit(X_scaled, y)\n",
    "            y_pred_full = model.predict(X_scaled)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'cv_scores': cv_scores,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'full_correlation': pearsonr(y, y_pred_full)[0],\n",
    "                'full_r2': r2_score(y, y_pred_full)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\\\n{model_name}:\")\n",
    "            print(f\"  CV correlations: {cv_scores.round(3)}\")\n",
    "            print(f\"  Mean CV r: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
    "            print(f\"  Full data r: {results[model_name]['full_correlation']:.3f}\")\n",
    "        \n",
    "        # Select best model\n",
    "        best_model_name = max(results.keys(), \n",
    "                            key=lambda x: results[x]['cv_mean'])\n",
    "        \n",
    "        print(f\"\\\\n{'='*50}\")\n",
    "        print(f\"BEST MODEL: {best_model_name}\")\n",
    "        print(f\"Mean CV correlation: {results[best_model_name]['cv_mean']:.3f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Store results\n",
    "        self.model_results = results\n",
    "        self.best_model = results[best_model_name]['model']\n",
    "        self.selected_features = top_features\n",
    "        \n",
    "        # Create visualization\n",
    "        self._plot_model_results(results, X_scaled, y, groups)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _select_top_features(self):\n",
    "        \"\"\"Select top features based on correlation with target.\"\"\"\n",
    "        correlations = {}\n",
    "        \n",
    "        for i, feat_name in enumerate(self.feature_names):\n",
    "            feat_data = self.combined_features[:, i]\n",
    "            if np.std(feat_data) > 1e-10:\n",
    "                corr, _ = pearsonr(feat_data, self.combined_targets)\n",
    "                correlations[feat_name] = abs(corr)\n",
    "        \n",
    "        # Sort by correlation\n",
    "        sorted_features = sorted(correlations.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top N features\n",
    "        n_features = self.params['features']['n_top_features']\n",
    "        top_features = [f[0] for f in sorted_features[:n_features]]\n",
    "        \n",
    "        print(f\"\\\\nTop {n_features} features:\")\n",
    "        for i, (feat, corr) in enumerate(sorted_features[:n_features]):\n",
    "            print(f\"{i+1:2d}. {feat:<40} |r|={corr:.3f}\")\n",
    "        \n",
    "        return top_features, correlations\n",
    "    \n",
    "    def _plot_model_results(self, results, X_scaled, y, groups):\n",
    "        \"\"\"Create visualization of model results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Multi-Subject Model Results', fontsize=16)\n",
    "        \n",
    "        # 1. Model comparison\n",
    "        ax = axes[0, 0]\n",
    "        model_names = list(results.keys())\n",
    "        cv_means = [results[m]['cv_mean'] for m in model_names]\n",
    "        cv_stds = [results[m]['cv_std'] for m in model_names]\n",
    "        \n",
    "        bars = ax.bar(model_names, cv_means, yerr=cv_stds, capsize=5)\n",
    "        ax.set_ylabel('Mean CV Correlation')\n",
    "        ax.set_title('Model Performance Comparison')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Highlight best model\n",
    "        best_idx = np.argmax(cv_means)\n",
    "        bars[best_idx].set_color('darkgreen')\n",
    "        \n",
    "        # 2. Subject-wise performance\n",
    "        ax = axes[0, 1]\n",
    "        best_model = self.best_model\n",
    "        \n",
    "        subject_correlations = {}\n",
    "        unique_subjects = np.unique(groups)\n",
    "        \n",
    "        for subject in unique_subjects:\n",
    "            subject_mask = groups == subject\n",
    "            X_subj = X_scaled[subject_mask]\n",
    "            y_subj = y[subject_mask]\n",
    "            \n",
    "            y_pred = best_model.predict(X_subj)\n",
    "            corr, _ = pearsonr(y_subj, y_pred)\n",
    "            subject_correlations[subject] = corr\n",
    "        \n",
    "        subjects = list(subject_correlations.keys())\n",
    "        correlations = list(subject_correlations.values())\n",
    "        \n",
    "        ax.bar(range(len(subjects)), correlations)\n",
    "        ax.set_xlabel('Subject')\n",
    "        ax.set_ylabel('Correlation')\n",
    "        ax.set_title('Per-Subject Performance (Best Model)')\n",
    "        ax.set_xticks(range(len(subjects)))\n",
    "        ax.set_xticklabels(subjects, rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 3. Feature importance (for tree-based models)\n",
    "        ax = axes[1, 0]\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            importances = self.best_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]\n",
    "            \n",
    "            top_features = [self.selected_features[i] for i in indices]\n",
    "            top_importances = importances[indices]\n",
    "            \n",
    "            y_pos = np.arange(len(top_features))\n",
    "            ax.barh(y_pos, top_importances)\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(top_features)\n",
    "            ax.set_xlabel('Importance')\n",
    "            ax.set_title('Top 10 Feature Importances')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Feature importance\\\\nnot available\\\\nfor this model',\n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # 4. Predictions scatter plot\n",
    "        ax = axes[1, 1]\n",
    "        y_pred = self.best_model.predict(X_scaled)\n",
    "        \n",
    "        # Downsample for clarity\n",
    "        downsample = 10\n",
    "        ax.scatter(y[::downsample], y_pred[::downsample], \n",
    "                  alpha=0.5, s=20)\n",
    "        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual PDA')\n",
    "        ax.set_ylabel('Predicted PDA')\n",
    "        ax.set_title(f'Predictions (r={pearsonr(y, y_pred)[0]:.3f})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'model_results.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def test_on_new_subject(self, subject_id, eeg_file, pda_file=None):\n",
    "        \"\"\"\n",
    "        Test the trained model on a new subject.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_id : str\n",
    "            Subject identifier\n",
    "        eeg_file : str\n",
    "            Path to EEG file\n",
    "        pda_file : str, optional\n",
    "            Path to PDA file (for evaluation)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Test results including predictions\n",
    "        \"\"\"\n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(f\"TESTING ON NEW SUBJECT: {subject_id}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Process the subject\n",
    "        test_data = self.process_single_subject(subject_id, eeg_file, pda_file)\n",
    "        \n",
    "        # Extract selected features\n",
    "        feature_indices = [test_data['feature_names'].index(f) \n",
    "                          for f in self.selected_features]\n",
    "        X_test = test_data['features'][:, feature_indices]\n",
    "        \n",
    "        # Scale features\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.best_model.predict(X_test_scaled)\n",
    "        \n",
    "        results = {\n",
    "            'subject_id': subject_id,\n",
    "            'predictions': predictions,\n",
    "            'time_points': test_data['time_points']\n",
    "        }\n",
    "        \n",
    "        # If true PDA provided, evaluate\n",
    "        if pda_file is not None:\n",
    "            y_true = test_data['target']\n",
    "            correlation, _ = pearsonr(y_true, predictions)\n",
    "            r2 = r2_score(y_true, predictions)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "            \n",
    "            results.update({\n",
    "                'true_pda': y_true,\n",
    "                'correlation': correlation,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            })\n",
    "            \n",
    "            print(f\"\\\\nTest Results:\")\n",
    "            print(f\"  Correlation: {correlation:.3f}\")\n",
    "            print(f\"  R²: {r2:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            \n",
    "            # Plot results\n",
    "            self._plot_test_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _plot_test_results(self, results):\n",
    "        \"\"\"Plot test results for a single subject.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        time_points = results['time_points']\n",
    "        predictions = results['predictions']\n",
    "        true_pda = results['true_pda']\n",
    "        \n",
    "        # Time series\n",
    "        ax = axes[0]\n",
    "        ax.plot(time_points, true_pda, 'k-', label='True PDA', alpha=0.8)\n",
    "        ax.plot(time_points, predictions, 'r--', label='Predicted PDA', alpha=0.8)\n",
    "        ax.set_ylabel('PDA (z-score)')\n",
    "        ax.set_title(f\"Test Subject: {results['subject_id']} (r={results['correlation']:.3f})\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residuals\n",
    "        ax = axes[1]\n",
    "        residuals = true_pda - predictions\n",
    "        ax.plot(time_points, residuals, 'g-', alpha=0.7)\n",
    "        ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        ax.set_title('Prediction Errors')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f\"test_{results['subject_id']}.png\"),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, filename='multi_subject_pda_model.pkl'):\n",
    "        \"\"\"Save the trained model and necessary components.\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.best_model,\n",
    "            'scaler': self.scaler,\n",
    "            'selected_features': self.selected_features,\n",
    "            'model_results': self.model_results,\n",
    "            'params': self.params,\n",
    "            'n_subjects_trained': len(np.unique(self.subject_ids))\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"\\\\nModel saved to: {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously trained model.\"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.best_model = model_data['model']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.selected_features = model_data['selected_features']\n",
    "        self.model_results = model_data['model_results']\n",
    "        self.params = model_data['params']\n",
    "        print(f\"Model loaded from: {filepath}\")\n",
    "        print(f\"Trained on {model_data['n_subjects_trained']} subjects\")\n",
    "'''\n",
    "    \n",
    "    create_file('multi_subject_pipeline.py', multi_subject_pipeline_content)\n",
    "    \n",
    "    # 2. Create automated_channel_cleaning.py (shortened version)\n",
    "    cleaning_content = '''\"\"\"\n",
    "Automated EEG Channel Cleaning Module\n",
    "====================================\n",
    "\"\"\"\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def automated_channel_cleaning(raw, ecg_channel_names=None, z_score_threshold=5.0, \n",
    "                              correlation_threshold=0.2, powerline_threshold=15.0,\n",
    "                              save_report=True, report_path=None):\n",
    "    \"\"\"\n",
    "    Simplified automated EEG channel cleaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Common ECG channel name patterns\n",
    "    ecg_patterns = ['ECG', 'EKG', 'ecg', 'ekg', 'ECG1', 'ECG2', 'EKG1', 'EKG2']\n",
    "    \n",
    "    # Find and remove ECG channels\n",
    "    ecg_channels_found = []\n",
    "    if ecg_channel_names is None:\n",
    "        for ch_name in raw.ch_names:\n",
    "            if any(pattern in ch_name for pattern in ecg_patterns):\n",
    "                ecg_channels_found.append(ch_name)\n",
    "    else:\n",
    "        ecg_channels_found = ecg_channel_names\n",
    "    \n",
    "    # Remove ECG channels\n",
    "    if ecg_channels_found:\n",
    "        print(f\"Removing ECG channels: {ecg_channels_found}\")\n",
    "        raw = raw.copy().drop_channels(ecg_channels_found)\n",
    "    else:\n",
    "        print(\"No ECG channels found to remove\")\n",
    "        raw = raw.copy()\n",
    "    \n",
    "    # Get data for analysis\n",
    "    data = raw.get_data()\n",
    "    sfreq = raw.info['sfreq']\n",
    "    \n",
    "    bad_channels = []\n",
    "    \n",
    "    # 1. Variance-based detection\n",
    "    channel_vars = np.var(data, axis=1)\n",
    "    z_scores = np.abs((channel_vars - np.median(channel_vars)) / np.median(channel_vars))\n",
    "    \n",
    "    bad_by_variance = []\n",
    "    for idx, (z_score, ch_name) in enumerate(zip(z_scores, raw.ch_names)):\n",
    "        if z_score > z_score_threshold:\n",
    "            bad_by_variance.append(ch_name)\n",
    "            print(f\"  - {ch_name}: High variance (z-score: {z_score:.2f})\")\n",
    "        elif channel_vars[idx] < np.median(channel_vars) * 0.01:\n",
    "            bad_by_variance.append(ch_name)\n",
    "            print(f\"  - {ch_name}: Nearly flat\")\n",
    "    \n",
    "    bad_channels.extend(bad_by_variance)\n",
    "    \n",
    "    # 2. Correlation-based detection\n",
    "    downsampled_data = data[:, ::10]\n",
    "    corr_matrix = np.corrcoef(downsampled_data)\n",
    "    \n",
    "    bad_by_correlation = []\n",
    "    for idx, ch_name in enumerate(raw.ch_names):\n",
    "        if ch_name in bad_channels:\n",
    "            continue\n",
    "        \n",
    "        correlations = corr_matrix[idx, :]\n",
    "        correlations[idx] = 0\n",
    "        mean_corr = np.mean(np.abs(correlations))\n",
    "        \n",
    "        if mean_corr < correlation_threshold:\n",
    "            bad_by_correlation.append(ch_name)\n",
    "            print(f\"  - {ch_name}: Low correlation (mean: {mean_corr:.2f})\")\n",
    "    \n",
    "    bad_channels.extend(bad_by_correlation)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    bad_channels = list(set(bad_channels))\n",
    "    \n",
    "    # Mark bad channels\n",
    "    raw.info['bads'] = bad_channels\n",
    "    \n",
    "    print(f\"\\\\nTotal bad channels detected: {len(bad_channels)}\")\n",
    "    \n",
    "    # Simple QC stats\n",
    "    qc_stats = {\n",
    "        'bad_channels': bad_channels,\n",
    "        'bad_by_variance': bad_by_variance,\n",
    "        'bad_by_correlation': bad_by_correlation,\n",
    "        'ecg_channels_removed': ecg_channels_found\n",
    "    }\n",
    "    \n",
    "    # Optional: Save a simple plot\n",
    "    if save_report and report_path:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(range(len(channel_vars)), channel_vars)\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('Variance')\n",
    "        ax.set_title('Channel Variance Distribution')\n",
    "        for i, ch in enumerate(bad_channels):\n",
    "            if ch in raw.ch_names:\n",
    "                idx = raw.ch_names.index(ch)\n",
    "                ax.bar(idx, channel_vars[idx], color='red')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(report_path)\n",
    "        plt.close()\n",
    "    \n",
    "    return raw, bad_channels, qc_stats\n",
    "'''\n",
    "    \n",
    "    create_file('automated_channel_cleaning.py', cleaning_content)\n",
    "    \n",
    "    # 3. Create lz_complexity.py (simplified version)\n",
    "    lz_content = '''\"\"\"\n",
    "Lempel-Ziv Complexity Module\n",
    "===========================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_lz_features_matlab_style(eeg_data, fs, window_length=1.0, overlap=0.5, \n",
    "                                   complexity_type='exhaustive', use_fast=True):\n",
    "    \"\"\"\n",
    "    Simplified Lempel-Ziv complexity computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_channels, n_samples = eeg_data.shape\n",
    "    window_samples = int(window_length * fs)\n",
    "    step_samples = int(window_samples * (1 - overlap))\n",
    "    n_windows = (n_samples - window_samples) // step_samples + 1\n",
    "    \n",
    "    print(f\"Computing LZ complexity ({complexity_type})...\")\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    lz_complexity = np.zeros((n_windows, n_channels))\n",
    "    lz_times = np.zeros(n_windows)\n",
    "    \n",
    "    # Pre-compute channel medians for binarization\n",
    "    channel_medians = np.median(eeg_data, axis=1)\n",
    "    \n",
    "    # Process windows\n",
    "    for win_idx in tqdm(range(n_windows), desc=f\"LZ {complexity_type}\"):\n",
    "        start = win_idx * step_samples\n",
    "        end = start + window_samples\n",
    "        \n",
    "        if end > n_samples:\n",
    "            break\n",
    "            \n",
    "        lz_times[win_idx] = start / fs + window_length / 2\n",
    "        \n",
    "        # Compute LZ for each channel\n",
    "        for ch in range(n_channels):\n",
    "            # Extract and binarize window\n",
    "            window_data = eeg_data[ch, start:end]\n",
    "            binary_seq = (window_data > channel_medians[ch]).astype(int)\n",
    "            \n",
    "            # Simplified LZ complexity calculation\n",
    "            binary_str = ''.join(binary_seq.astype(str))\n",
    "            \n",
    "            # Count unique substrings (simplified approach)\n",
    "            substrings = set()\n",
    "            max_len = min(10, len(binary_str) // 4)\n",
    "            \n",
    "            for length in range(1, max_len + 1):\n",
    "                for i in range(len(binary_str) - length + 1):\n",
    "                    substrings.add(binary_str[i:i+length])\n",
    "            \n",
    "            c = len(substrings)\n",
    "            if len(binary_str) > 1:\n",
    "                c_norm = c / (len(binary_str) / np.log2(len(binary_str)))\n",
    "            else:\n",
    "                c_norm = c\n",
    "            \n",
    "            lz_complexity[win_idx, ch] = c_norm\n",
    "    \n",
    "    # Trim to actual number of windows\n",
    "    actual_windows = win_idx\n",
    "    \n",
    "    features = {\n",
    "        'lz_complexity_mean': np.mean(lz_complexity[:actual_windows], axis=1),\n",
    "        'lz_complexity_std': np.std(lz_complexity[:actual_windows], axis=1),\n",
    "        'lz_complexity_max': np.max(lz_complexity[:actual_windows], axis=1),\n",
    "        'lz_complexity_min': np.min(lz_complexity[:actual_windows], axis=1),\n",
    "        'lz_times': lz_times[:actual_windows]\n",
    "    }\n",
    "    \n",
    "    # Add spatial features if enough channels\n",
    "    if n_channels >= 20:\n",
    "        features['lz_complexity_frontal'] = np.mean(lz_complexity[:actual_windows, :10], axis=1)\n",
    "        features['lz_complexity_posterior'] = np.mean(lz_complexity[:actual_windows, -10:], axis=1)\n",
    "        features['lz_frontal_posterior_diff'] = (\n",
    "            features['lz_complexity_frontal'] - features['lz_complexity_posterior']\n",
    "        )\n",
    "    \n",
    "    return features\n",
    "'''\n",
    "    \n",
    "    create_file('lz_complexity.py', lz_content)\n",
    "    \n",
    "    # 4. Create the quick start script (your actual usage script)\n",
    "    quick_start = open('quick_start.py', 'r').read() if os.path.exists('quick_start.py') else '''\"\"\"\n",
    "Quick Start Script\n",
    "==================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from multi_subject_pipeline import MultiSubjectPDAPipeline\n",
    "\n",
    "# Create subject list\n",
    "subject_runs = []\n",
    "edf_files = sorted(glob.glob('../edfs/sub-*_task-feedback-run*.edf'))\n",
    "\n",
    "for edf_file in edf_files:\n",
    "    basename = os.path.basename(edf_file)\n",
    "    parts = basename.split('_')\n",
    "    subject_id = parts[0]\n",
    "    run_num = basename.split('-run')[1].replace('.edf', '')\n",
    "    \n",
    "    task_file = f'./task_output/{subject_id}_DMN_Feedback_run{run_num}_roi_outputs.csv'\n",
    "    \n",
    "    if os.path.exists(task_file):\n",
    "        subject_run_id = f'{subject_id}_run{run_num}'\n",
    "        subject_runs.append((subject_run_id, edf_file, task_file))\n",
    "        print(f\"Found: {subject_run_id}\")\n",
    "\n",
    "print(f\"\\\\nTotal runs found: {len(subject_runs)}\")\n",
    "\n",
    "# Split into train/test\n",
    "subjects = {}\n",
    "for subject_run_id, edf, task in subject_runs:\n",
    "    subject = subject_run_id.split('_run')[0]\n",
    "    if subject not in subjects:\n",
    "        subjects[subject] = []\n",
    "    subjects[subject].append((subject_run_id, edf, task))\n",
    "\n",
    "subject_list = list(subjects.keys())\n",
    "n_train = int(0.8 * len(subject_list))\n",
    "\n",
    "train_subjects = subject_list[:n_train]\n",
    "test_subjects = subject_list[n_train:]\n",
    "\n",
    "train_runs = []\n",
    "test_runs = []\n",
    "\n",
    "for subject in train_subjects:\n",
    "    train_runs.extend(subjects[subject])\n",
    "\n",
    "for subject in test_subjects:\n",
    "    test_runs.extend(subjects[subject])\n",
    "\n",
    "print(f\"Training: {len(train_subjects)} subjects, {len(train_runs)} runs\")\n",
    "print(f\"Testing: {len(test_subjects)} subjects, {len(test_runs)} runs\")\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = MultiSubjectPDAPipeline(\n",
    "    base_dir='../',\n",
    "    output_dir='./results_quickstart'\n",
    ")\n",
    "\n",
    "print(\"\\\\nProcessing training subjects...\")\n",
    "pipeline.process_all_subjects(train_runs)\n",
    "\n",
    "print(\"\\\\nTraining models...\")\n",
    "pipeline.combine_subjects_data()\n",
    "results = pipeline.train_models()\n",
    "\n",
    "pipeline.save_model('quickstart_model.pkl')\n",
    "\n",
    "print(\"\\\\nPipeline complete!\")\n",
    "'''\n",
    "    \n",
    "    create_file('run_pipeline.py', quick_start)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Setup complete!\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nCreated files:\")\n",
    "    print(\"  - multi_subject_pipeline.py (main pipeline)\")\n",
    "    print(\"  - automated_channel_cleaning.py (EEG cleaning)\")\n",
    "    print(\"  - lz_complexity.py (LZ features)\")\n",
    "    print(\"  - run_pipeline.py (ready to run)\")\n",
    "    print(\"\\nTo start processing your data, run:\")\n",
    "    print(\"  python run_pipeline.py\")\n",
    "    print(\"\\nNote: Make sure you have all required packages installed:\")\n",
    "    print(\"  pip install mne numpy pandas scipy scikit-learn xgboost tqdm matplotlib seaborn joblib\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_pipeline_files()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:analysis/MNE/DMNELF_multisubject_pipeline_250814.ipynb
   "execution_count": 2,
   "id": "f358e4a8",
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multi_subject_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7ec4c1850619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmulti_subject_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiSubjectPDAPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'multi_subject_pipeline'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> f2d28158e59dec2435ac8d2d3ddc6555b838b99c:analysis/MNE/DMNELF_multisubject_pipeline_250813.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROCESSING 5 SUBJECTS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf001\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf001: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf004\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf004: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf005\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf005: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf006\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf006: No module named 'automated_channel_cleaning'\n",
      "\n",
      "======================================================================\n",
      "Processing Subject: sub-dmnelf007\n",
      "======================================================================\n",
      "\n",
      "1. Loading and cleaning EEG data...\n",
      "\n",
      "Error processing subject sub-dmnelf007: No module named 'automated_channel_cleaning'\n",
      "\n",
      "\n",
      "Successfully processed 0 subjects\n",
      "\n",
      "======================================================================\n",
      "COMBINING MULTI-SUBJECT DATA\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mprocess_all_subjects(training_subjects)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Combine data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mcombine_subjects_data()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m     26\u001b[0m results \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtrain_models()\n",
      "Cell \u001b[0;32mIn[1], line 450\u001b[0m, in \u001b[0;36mMultiSubjectPDAPipeline.combine_subjects_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m all_subject_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Get common feature names (use first subject as reference)\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m ref_subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects_data\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    451\u001b[0m common_features \u001b[38;5;241m=\u001b[39m ref_subject[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject_id, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# Ensure feature consistency\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = MultiSubjectPDAPipeline(\n",
    "        base_dir='./data',\n",
    "        output_dir='./results_multi_subject'\n",
    "    )\n",
    "    \n",
    "    # Example: Process training subjects\n",
    "    training_subjects = [\n",
    "        ('sub-dmnelf001', '../edfs/sub-dmnelf001_task-feedback-run01.edf', './task_output/sub-dmnelf001_DMN_Feedback_run01_roi_outputs.csv'),\n",
    "        ('sub-dmnelf004', './edfs/sub02_eeg.edf', './data/sub02_pda.csv'),\n",
    "        ('sub-dmnelf005', './edfs/sub03_eeg.edf', './data/sub03_pda.csv'),\n",
    "        ('sub-dmnelf006', './edfs/sub03_eeg.edf', './data/sub03_pda.csv'),\n",
    "        ('sub-dmnelf007', './edfs/sub03_eeg.edf', './data/sub03_pda.csv'),\n",
    "        # Add more subjects...\n",
    "    ]\n",
    "    \n",
    "    # Process all subjects\n",
    "    pipeline.process_all_subjects(training_subjects)\n",
    "    \n",
    "    # Combine data\n",
    "    pipeline.combine_subjects_data()\n",
    "    \n",
    "    # Train models\n",
    "    results = pipeline.train_models()\n",
    "    \n",
    "    # Save model\n",
    "    pipeline.save_model()\n",
    "    \n",
    "    # Test on new subject\n",
    "    test_results = pipeline.test_on_new_subject(\n",
    "        'sub_test',\n",
    "        './data/sub_test_eeg.edf',\n",
    "        './data/sub_test_pda.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33dd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
