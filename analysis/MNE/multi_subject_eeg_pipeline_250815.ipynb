{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26db0290",
   "metadata": {},
   "source": [
    "# Multi-Subject EEG-fMRI Prediction Pipeline\n",
    "\n",
    "## Cell 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import mne\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy.signal import stft\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "EDF_PATH = '../edfs/'\n",
    "CSV_PATH = '../task_output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c1069",
   "metadata": {},
   "source": [
    "## Cell 2: Data Loading and Organization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc8a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No CSV found for sub-dmnelf006_task-feedback-run02.edf\n",
      "Warning: No CSV found for sub-dmnelf006_task-feedback-run03.edf\n",
      "Warning: No CSV found for sub-dmnelf006_task-feedback-run01.edf\n",
      "Warning: No CSV found for sub-dmnelf006_task-feedback-run04.edf\n",
      "Warning: No CSV found for sub-dmnelf001_task-feedback-run01.edf\n",
      "Warning: No CSV found for sub-dmnelf001_task-feedback-run03.edf\n",
      "Warning: No CSV found for sub-dmnelf005_task-feedback-run04.edf\n",
      "Warning: No CSV found for sub-dmnelf001_task-feedback-run02.edf\n",
      "Warning: No CSV found for sub-dmnelf005_task-feedback-run01.edf\n",
      "Warning: No CSV found for sub-dmnelf005_task-feedback-run02.edf\n",
      "Warning: No CSV found for sub-dmnelf005_task-feedback-run03.edf\n",
      "Warning: No CSV found for sub-dmnelf001_task-feedback-run04.edf\n",
      "Warning: No CSV found for sub-rtbpd003_task-feedback-run01.edf\n",
      "Warning: No CSV found for sub-dmnelf007_task-feedback-run04.edf\n",
      "Warning: No CSV found for sub-dmnelf007_task-feedback-run01.edf\n",
      "Warning: No CSV found for sub-dmnelf007_task-feedback-run03.edf\n",
      "Warning: No CSV found for sub-dmnelf007_task-feedback-run02.edf\n",
      "Warning: No CSV found for sub-dmnelf004_task-feedback-run03.edf\n",
      "Warning: No CSV found for sub-dmnelf004_task-feedback-run02.edf\n",
      "Warning: No CSV found for sub-dmnelf004_task-feedback-run01.edf\n",
      "Warning: No CSV found for sub-dmnelf004_task-feedback-run04.edf\n",
      "======================================================================\n",
      "DATA LOADING SUMMARY\n",
      "======================================================================\n",
      "Total subjects found: 0\n",
      "\n",
      "======================================================================\n",
      "SUBJECT SELECTION\n",
      "======================================================================\n",
      "\n",
      "Available subjects:\n",
      "\n",
      "Select option:\n",
      "1. Use all subjects with leave-one-out\n",
      "2. Select specific train/test split\n",
      "3. Select subset of subjects\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m subjects_data \u001b[38;5;241m=\u001b[39m load_all_data_files()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Select subjects\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m selected_subjects, test_mode, test_subjects \u001b[38;5;241m=\u001b[39m select_subjects_for_analysis(subjects_data)\n",
      "Cell \u001b[0;32mIn[2], line 79\u001b[0m, in \u001b[0;36mselect_subjects_for_analysis\u001b[0;34m(subjects_data)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Select specific train/test split\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Select subset of subjects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnter choice (1-3) [default=1]: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Use all subjects\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     selected_subjects \u001b[38;5;241m=\u001b[39m all_subjects\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def load_all_data_files():\n",
    "    \"\"\"\n",
    "    Load all EDF and CSV files and organize by subject and run.\n",
    "    \"\"\"\n",
    "    # Get all EDF files\n",
    "    edf_files = glob.glob(os.path.join(EDF_PATH, '*.edf'))\n",
    "    csv_files = glob.glob(os.path.join(CSV_PATH, '*.csv'))\n",
    "    \n",
    "    print(f\"Found {len(edf_files)} EDF files and {len(csv_files)} CSV files\")\n",
    "    \n",
    "    # Extract subject info\n",
    "    subjects_data = {}\n",
    "    \n",
    "    for edf_file in edf_files:\n",
    "        # Extract subject ID and run\n",
    "        basename = os.path.basename(edf_file)\n",
    "        \n",
    "        # Extract subject ID (e.g., 'dmnelf001' from 'sub-dmnelf001_task-feedback-run01.edf')\n",
    "        if 'sub-' in basename:\n",
    "            subject_id = basename.split('_')[0].replace('sub-', '')\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Extract run number from format like 'run01' or 'run-01'\n",
    "        import re\n",
    "        run_match = re.search(r'run[-]?(\\d+)', basename)\n",
    "        if run_match:\n",
    "            run_num = run_match.group(1).zfill(2)  # Ensure 2 digits\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Find matching CSV file\n",
    "        # Your CSV format: sub-dmnelf001_DMN_Feedback_run01_roi_outputs.csv\n",
    "        # Try different patterns\n",
    "        possible_patterns = [\n",
    "            f\"sub-{subject_id}_DMN_Feedback_run{run_num}_roi_outputs.csv\",\n",
    "            f\"sub-{subject_id}_*_run{run_num}_roi_outputs.csv\",\n",
    "            f\"sub-{subject_id}_*run{run_num}*.csv\"\n",
    "        ]\n",
    "        \n",
    "        matching_csv = None\n",
    "        for pattern in possible_patterns:\n",
    "            for csv_file in csv_files:\n",
    "                csv_basename = os.path.basename(csv_file)\n",
    "                # Direct match\n",
    "                if csv_basename == pattern.replace('*', 'DMN_Feedback_'):\n",
    "                    matching_csv = csv_file\n",
    "                    break\n",
    "                # Pattern match for wildcards\n",
    "                if '*' in pattern:\n",
    "                    pattern_regex = pattern.replace('*', '.*')\n",
    "                    if re.match(pattern_regex, csv_basename):\n",
    "                        matching_csv = csv_file\n",
    "                        break\n",
    "            if matching_csv:\n",
    "                break\n",
    "        \n",
    "        if not matching_csv:\n",
    "            print(f\"Warning: No CSV found for {basename}\")\n",
    "            print(f\"  Looked for patterns: {possible_patterns}\")\n",
    "            continue\n",
    "            \n",
    "        # Initialize subject data structure\n",
    "        if subject_id not in subjects_data:\n",
    "            subjects_data[subject_id] = {}\n",
    "            \n",
    "        # Store file paths\n",
    "        subjects_data[subject_id][f'run{run_num}'] = {\n",
    "            'edf': edf_file,\n",
    "            'csv': matching_csv\n",
    "        }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA LOADING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total subjects found: {len(subjects_data)}\")\n",
    "    \n",
    "    for subject_id, runs in subjects_data.items():\n",
    "        print(f\"\\n{subject_id}:\")\n",
    "        for run, files in runs.items():\n",
    "            print(f\"  {run}: EDF ✓, CSV ✓\")\n",
    "    \n",
    "    return subjects_data\n",
    "\n",
    "\n",
    "def select_subjects_for_analysis(subjects_data):\n",
    "    \"\"\"\n",
    "    Interactive selection of subjects for training and testing.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUBJECT SELECTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_subjects = list(subjects_data.keys())\n",
    "    print(\"\\nAvailable subjects:\")\n",
    "    for i, subj in enumerate(all_subjects):\n",
    "        n_runs = len(subjects_data[subj])\n",
    "        print(f\"{i+1}. {subj} ({n_runs} runs)\")\n",
    "    \n",
    "    # Option 1: Select specific subjects\n",
    "    print(\"\\nSelect option:\")\n",
    "    print(\"1. Use all subjects with leave-one-out\")\n",
    "    print(\"2. Select specific train/test split\")\n",
    "    print(\"3. Select subset of subjects\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1-3) [default=1]: \").strip() or \"1\"\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        # Use all subjects\n",
    "        selected_subjects = all_subjects\n",
    "        test_mode = 'leave_one_out'\n",
    "        test_subjects = None\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        # Manual train/test split\n",
    "        print(\"\\nSelect TRAINING subjects (comma-separated numbers):\")\n",
    "        train_idx = input(\"Training subjects: \").strip()\n",
    "        train_idx = [int(i)-1 for i in train_idx.split(',')]\n",
    "        train_subjects = [all_subjects[i] for i in train_idx]\n",
    "        \n",
    "        print(\"\\nSelect TEST subjects (comma-separated numbers):\")\n",
    "        test_idx = input(\"Test subjects: \").strip()\n",
    "        test_idx = [int(i)-1 for i in test_idx.split(',')]\n",
    "        test_subjects = [all_subjects[i] for i in test_idx]\n",
    "        \n",
    "        selected_subjects = train_subjects + test_subjects\n",
    "        test_mode = 'manual_split'\n",
    "        \n",
    "    else:\n",
    "        # Select subset\n",
    "        print(\"\\nSelect subjects to include (comma-separated numbers):\")\n",
    "        subset_idx = input(\"Subjects: \").strip()\n",
    "        subset_idx = [int(i)-1 for i in subset_idx.split(',')]\n",
    "        selected_subjects = [all_subjects[i] for i in subset_idx]\n",
    "        test_mode = 'leave_one_out'\n",
    "        test_subjects = None\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_subjects)} subjects for analysis\")\n",
    "    print(f\"Test mode: {test_mode}\")\n",
    "    \n",
    "    return selected_subjects, test_mode, test_subjects\n",
    "\n",
    "\n",
    "# Load all data\n",
    "subjects_data = load_all_data_files()\n",
    "\n",
    "# Select subjects\n",
    "selected_subjects, test_mode, test_subjects = select_subjects_for_analysis(subjects_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d8532",
   "metadata": {},
   "source": [
    "## Cell 3: Import Single-Subject Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465922be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the processing functions from your single-subject pipeline\n",
    "# (You can copy these from your original notebook or import from a module)\n",
    "\n",
    "# Include these functions:\n",
    "# - automated_channel_cleaning\n",
    "# - process_eeg_file\n",
    "# - align_pda_to_eeg\n",
    "# - compute_lz_features_matlab_style\n",
    "# - extract_advanced_eeg_features\n",
    "# - etc.\n",
    "\n",
    "# For now, I'll create a placeholder\n",
    "print(\"Import your single-subject processing functions here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9604fa5",
   "metadata": {},
   "source": [
    "## Cell 4: Multi-Subject Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d484ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_data(subject_id, runs_data, processing_params):\n",
    "    \"\"\"\n",
    "    Process all runs for a single subject.\n",
    "    \"\"\"\n",
    "    subject_features = []\n",
    "    subject_targets = []\n",
    "    subject_metadata = []\n",
    "    \n",
    "    print(f\"\\nProcessing subject: {subject_id}\")\n",
    "    \n",
    "    for run_id, file_paths in runs_data.items():\n",
    "        print(f\"  Processing {run_id}...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Load and clean EEG data\n",
    "            raw = mne.io.read_raw_edf(file_paths['edf'], preload=True)\n",
    "            \n",
    "            # Apply automated cleaning\n",
    "            raw_processed, bad_channels, qc_stats = automated_channel_cleaning(\n",
    "                raw, \n",
    "                z_score_threshold=processing_params['z_score_threshold'],\n",
    "                correlation_threshold=processing_params['correlation_threshold'],\n",
    "                powerline_threshold=processing_params['powerline_threshold'],\n",
    "                save_report=False\n",
    "            )\n",
    "            \n",
    "            # 2. Load PDA data\n",
    "            pda_df = pd.read_csv(file_paths['csv'])\n",
    "            if 'cen' in pda_df.columns and 'dmn' in pda_df.columns:\n",
    "                pda_signal = (pda_df['cen'] - pda_df['dmn']).values\n",
    "            else:\n",
    "                print(f\"    Warning: Missing CEN/DMN columns in {run_id}\")\n",
    "                continue\n",
    "            \n",
    "            # 3. Align PDA to EEG\n",
    "            pda_aligned_z, pda_time_aligned = align_pda_to_eeg(\n",
    "                pda_signal, raw_processed,\n",
    "                hrf_delay=processing_params['hrf_delay'],\n",
    "                target_fs=processing_params['target_fs']\n",
    "            )\n",
    "            \n",
    "            # 4. Extract features\n",
    "            # Get EEG data\n",
    "            picks_eeg = mne.pick_types(raw_processed.info, eeg=True)\n",
    "            eeg_data = raw_processed.get_data(picks=picks_eeg)\n",
    "            fs = raw_processed.info['sfreq']\n",
    "            \n",
    "            # Compute LZ complexity features\n",
    "            lz_features = compute_lz_features_matlab_style(\n",
    "                eeg_data, fs,\n",
    "                window_length=processing_params['lz_window_length'],\n",
    "                overlap=processing_params['lz_overlap'],\n",
    "                complexity_type='exhaustive',\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            # Compute STFT features\n",
    "            nperseg = int(processing_params['stft_window'] * fs)\n",
    "            noverlap = int(processing_params['stft_overlap'] * fs)\n",
    "            \n",
    "            f, t_stft, Zxx = stft(eeg_data, fs=fs, nperseg=nperseg, \n",
    "                                  noverlap=noverlap, axis=1)\n",
    "            power = np.abs(Zxx) ** 2\n",
    "            power = power.transpose(2, 0, 1)\n",
    "            \n",
    "            # Extract advanced features\n",
    "            advanced_features = extract_advanced_eeg_features(\n",
    "                power, f, t_stft, raw_processed, \n",
    "                pda_aligned_z, pda_time_aligned\n",
    "            )\n",
    "            \n",
    "            # 5. Combine all features\n",
    "            run_features = {\n",
    "                **lz_features['aligned_features'],\n",
    "                **advanced_features['aligned_features']\n",
    "            }\n",
    "            \n",
    "            # Create feature matrix\n",
    "            feature_names = list(run_features.keys())\n",
    "            X_run = np.column_stack([run_features[feat] for feat in feature_names])\n",
    "            \n",
    "            # Store results\n",
    "            subject_features.append(X_run)\n",
    "            subject_targets.append(pda_aligned_z)\n",
    "            subject_metadata.append({\n",
    "                'subject_id': subject_id,\n",
    "                'run_id': run_id,\n",
    "                'n_samples': len(pda_aligned_z),\n",
    "                'feature_names': feature_names,\n",
    "                'bad_channels': bad_channels,\n",
    "                'n_good_channels': len(picks_eeg)\n",
    "            })\n",
    "            \n",
    "            print(f\"    ✓ Extracted {X_run.shape[1]} features, {X_run.shape[0]} samples\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Error processing {run_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        'features': subject_features,\n",
    "        'targets': subject_targets,\n",
    "        'metadata': subject_metadata\n",
    "    }\n",
    "\n",
    "\n",
    "# Define processing parameters\n",
    "processing_params = {\n",
    "    'z_score_threshold': 5.0,\n",
    "    'correlation_threshold': 0.2,\n",
    "    'powerline_threshold': 15.0,\n",
    "    'hrf_delay': 5.0,\n",
    "    'target_fs': 1.0,\n",
    "    'lz_window_length': 2.0,\n",
    "    'lz_overlap': 0.5,\n",
    "    'stft_window': 1.0,\n",
    "    'stft_overlap': 0.5\n",
    "}\n",
    "\n",
    "# Process all selected subjects\n",
    "all_subjects_data = {}\n",
    "\n",
    "for subject_id in tqdm(selected_subjects, desc=\"Processing subjects\"):\n",
    "    if subject_id in subjects_data:\n",
    "        subject_results = process_subject_data(\n",
    "            subject_id, \n",
    "            subjects_data[subject_id],\n",
    "            processing_params\n",
    "        )\n",
    "        all_subjects_data[subject_id] = subject_results\n",
    "\n",
    "print(f\"\\n✓ Processed {len(all_subjects_data)} subjects successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e778d",
   "metadata": {},
   "source": [
    "## Cell 5: Feature Alignment and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_features_across_subjects(all_subjects_data):\n",
    "    \"\"\"\n",
    "    Ensure all subjects have the same features in the same order.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE ALIGNMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Collect all unique feature names\n",
    "    all_feature_names = set()\n",
    "    \n",
    "    for subject_id, subject_data in all_subjects_data.items():\n",
    "        for metadata in subject_data['metadata']:\n",
    "            all_feature_names.update(metadata['feature_names'])\n",
    "    \n",
    "    # Sort for consistency\n",
    "    common_features = sorted(list(all_feature_names))\n",
    "    print(f\"Total unique features across all subjects: {len(common_features)}\")\n",
    "    \n",
    "    # Align features for each subject/run\n",
    "    aligned_data = {}\n",
    "    \n",
    "    for subject_id, subject_data in all_subjects_data.items():\n",
    "        aligned_features = []\n",
    "        aligned_targets = []\n",
    "        aligned_metadata = []\n",
    "        \n",
    "        for run_idx, (features, target, metadata) in enumerate(zip(\n",
    "            subject_data['features'],\n",
    "            subject_data['targets'],\n",
    "            subject_data['metadata']\n",
    "        )):\n",
    "            # Create feature dictionary for this run\n",
    "            run_features = dict(zip(metadata['feature_names'], features.T))\n",
    "            \n",
    "            # Create aligned feature matrix\n",
    "            aligned_X = []\n",
    "            for feat_name in common_features:\n",
    "                if feat_name in run_features:\n",
    "                    aligned_X.append(run_features[feat_name])\n",
    "                else:\n",
    "                    # Fill missing features with zeros\n",
    "                    aligned_X.append(np.zeros(len(target)))\n",
    "            \n",
    "            aligned_X = np.column_stack(aligned_X)\n",
    "            \n",
    "            aligned_features.append(aligned_X)\n",
    "            aligned_targets.append(target)\n",
    "            \n",
    "            # Update metadata\n",
    "            metadata['feature_names'] = common_features\n",
    "            aligned_metadata.append(metadata)\n",
    "        \n",
    "        aligned_data[subject_id] = {\n",
    "            'features': aligned_features,\n",
    "            'targets': aligned_targets,\n",
    "            'metadata': aligned_metadata\n",
    "        }\n",
    "    \n",
    "    return aligned_data, common_features\n",
    "\n",
    "\n",
    "# Align features\n",
    "aligned_data, common_features = align_features_across_subjects(all_subjects_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e8940",
   "metadata": {},
   "source": [
    "## Cell 6: Create Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(aligned_data, test_mode, test_subjects=None):\n",
    "    \"\"\"\n",
    "    Create train/test sets based on the selected mode.\n",
    "    \"\"\"\n",
    "    if test_mode == 'leave_one_out':\n",
    "        # Create leave-one-subject-out splits\n",
    "        splits = []\n",
    "        \n",
    "        for test_subject in aligned_data.keys():\n",
    "            train_subjects = [s for s in aligned_data.keys() if s != test_subject]\n",
    "            \n",
    "            # Combine training data\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            train_groups = []\n",
    "            \n",
    "            for subj_idx, train_subj in enumerate(train_subjects):\n",
    "                for run_features, run_target in zip(\n",
    "                    aligned_data[train_subj]['features'],\n",
    "                    aligned_data[train_subj]['targets']\n",
    "                ):\n",
    "                    X_train.append(run_features)\n",
    "                    y_train.append(run_target)\n",
    "                    train_groups.extend([subj_idx] * len(run_target))\n",
    "            \n",
    "            X_train = np.vstack(X_train)\n",
    "            y_train = np.hstack(y_train)\n",
    "            train_groups = np.array(train_groups)\n",
    "            \n",
    "            # Test data\n",
    "            X_test = []\n",
    "            y_test = []\n",
    "            \n",
    "            for run_features, run_target in zip(\n",
    "                aligned_data[test_subject]['features'],\n",
    "                aligned_data[test_subject]['targets']\n",
    "            ):\n",
    "                X_test.append(run_features)\n",
    "                y_test.append(run_target)\n",
    "            \n",
    "            X_test = np.vstack(X_test)\n",
    "            y_test = np.hstack(y_test)\n",
    "            \n",
    "            splits.append({\n",
    "                'test_subject': test_subject,\n",
    "                'train_subjects': train_subjects,\n",
    "                'X_train': X_train,\n",
    "                'y_train': y_train,\n",
    "                'X_test': X_test,\n",
    "                'y_test': y_test,\n",
    "                'train_groups': train_groups\n",
    "            })\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    elif test_mode == 'manual_split':\n",
    "        # Single train/test split\n",
    "        train_subjects = [s for s in aligned_data.keys() if s not in test_subjects]\n",
    "        \n",
    "        # Combine training data\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        for train_subj in train_subjects:\n",
    "            for run_features, run_target in zip(\n",
    "                aligned_data[train_subj]['features'],\n",
    "                aligned_data[train_subj]['targets']\n",
    "            ):\n",
    "                X_train.append(run_features)\n",
    "                y_train.append(run_target)\n",
    "        \n",
    "        X_train = np.vstack(X_train)\n",
    "        y_train = np.hstack(y_train)\n",
    "        \n",
    "        # Test data\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        \n",
    "        for test_subj in test_subjects:\n",
    "            for run_features, run_target in zip(\n",
    "                aligned_data[test_subj]['features'],\n",
    "                aligned_data[test_subj]['targets']\n",
    "            ):\n",
    "                X_test.append(run_features)\n",
    "                y_test.append(run_target)\n",
    "        \n",
    "        X_test = np.vstack(X_test)\n",
    "        y_test = np.hstack(y_test)\n",
    "        \n",
    "        return [{\n",
    "            'test_subjects': test_subjects,\n",
    "            'train_subjects': train_subjects,\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }]\n",
    "\n",
    "\n",
    "# Create train/test splits\n",
    "splits = create_train_test_sets(aligned_data, test_mode, test_subjects)\n",
    "print(f\"\\nCreated {len(splits)} train/test splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09b3c2",
   "metadata": {},
   "source": [
    "## Cell 7: Feature Selection and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features_multi(X_train, y_train, feature_names, n_features=20):\n",
    "    \"\"\"\n",
    "    Select top features based on correlation with target across training data.\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        feat_data = X_train[:, i]\n",
    "        if np.std(feat_data) > 1e-10:\n",
    "            corr, _ = pearsonr(feat_data, y_train)\n",
    "            correlations[i] = abs(corr)\n",
    "        else:\n",
    "            correlations[i] = 0\n",
    "    \n",
    "    # Sort by correlation\n",
    "    sorted_idx = sorted(correlations.keys(), key=lambda x: correlations[x], reverse=True)\n",
    "    top_idx = sorted_idx[:n_features]\n",
    "    top_features = [feature_names[i] for i in top_idx]\n",
    "    \n",
    "    return top_idx, top_features, correlations\n",
    "\n",
    "\n",
    "def train_multi_subject_models(splits, common_features, n_features=20):\n",
    "    \"\"\"\n",
    "    Train models for each split and evaluate performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-SUBJECT MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for split_idx, split in enumerate(splits):\n",
    "        if 'test_subject' in split:\n",
    "            print(f\"\\nFold {split_idx + 1}: Testing on {split['test_subject']}\")\n",
    "        else:\n",
    "            print(f\"\\nTesting on: {split['test_subjects']}\")\n",
    "        \n",
    "        # Feature selection on training data\n",
    "        top_idx, top_features, _ = select_top_features_multi(\n",
    "            split['X_train'], split['y_train'], common_features, n_features\n",
    "        )\n",
    "        \n",
    "        # Select features\n",
    "        X_train_selected = split['X_train'][:, top_idx]\n",
    "        X_test_selected = split['X_test'][:, top_idx]\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        # Train models\n",
    "        models = {\n",
    "            'Ridge': Ridge(alpha=1.0),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=2000),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, \n",
    "                                                 random_state=42, n_jobs=-1),\n",
    "            'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=3, \n",
    "                                       learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "        }\n",
    "        \n",
    "        fold_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            # Train\n",
    "            model.fit(X_train_scaled, split['y_train'])\n",
    "            \n",
    "            # Predict\n",
    "            y_pred_train = model.predict(X_train_scaled)\n",
    "            y_pred_test = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Evaluate\n",
    "            train_corr, _ = pearsonr(split['y_train'], y_pred_train)\n",
    "            test_corr, _ = pearsonr(split['y_test'], y_pred_test)\n",
    "            test_r2 = r2_score(split['y_test'], y_pred_test)\n",
    "            test_rmse = np.sqrt(mean_squared_error(split['y_test'], y_pred_test))\n",
    "            \n",
    "            fold_results[model_name] = {\n",
    "                'train_corr': train_corr,\n",
    "                'test_corr': test_corr,\n",
    "                'test_r2': test_r2,\n",
    "                'test_rmse': test_rmse,\n",
    "                'predictions': y_pred_test,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"  {model_name}: test r={test_corr:.3f}, R²={test_r2:.3f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'split_info': split,\n",
    "            'top_features': top_features,\n",
    "            'scaler': scaler,\n",
    "            'results': fold_results\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Train models\n",
    "model_results = train_multi_subject_models(splits, common_features, n_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42968d4",
   "metadata": {},
   "source": [
    "## Cell 8: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multi_subject_results(model_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of multi-subject results.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Model performance across subjects\n",
    "    ax1 = plt.subplot(3, 2, 1)\n",
    "    \n",
    "    model_names = list(model_results[0]['results'].keys())\n",
    "    n_splits = len(model_results)\n",
    "    \n",
    "    # Collect correlations\n",
    "    correlations = {model: [] for model in model_names}\n",
    "    \n",
    "    for split_result in model_results:\n",
    "        for model_name in model_names:\n",
    "            correlations[model_name].append(\n",
    "                split_result['results'][model_name]['test_corr']\n",
    "            )\n",
    "    \n",
    "    # Box plot\n",
    "    positions = np.arange(len(model_names))\n",
    "    bp = ax1.boxplot([correlations[m] for m in model_names], \n",
    "                     positions=positions, widths=0.6)\n",
    "    \n",
    "    ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Test Correlation')\n",
    "    ax1.set_title('Model Performance Across Subjects')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean values\n",
    "    means = [np.mean(correlations[m]) for m in model_names]\n",
    "    ax1.scatter(positions, means, color='red', s=100, zorder=3, label='Mean')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Subject-specific performance (best model)\n",
    "    ax2 = plt.subplot(3, 2, 2)\n",
    "    \n",
    "    # Find best model\n",
    "    mean_corrs = {m: np.mean(correlations[m]) for m in model_names}\n",
    "    best_model = max(mean_corrs.keys(), key=lambda x: mean_corrs[x])\n",
    "    \n",
    "    # Plot individual subject results\n",
    "    if 'test_subject' in model_results[0]['split_info']:\n",
    "        subjects = [r['split_info']['test_subject'] for r in model_results]\n",
    "        subject_corrs = [r['results'][best_model]['test_corr'] for r in model_results]\n",
    "        \n",
    "        bars = ax2.bar(range(len(subjects)), subject_corrs, \n",
    "                       color='skyblue', edgecolor='navy')\n",
    "        ax2.set_xticks(range(len(subjects)))\n",
    "        ax2.set_xticklabels(subjects, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Test Correlation')\n",
    "        ax2.set_title(f'Per-Subject Performance ({best_model})')\n",
    "        ax2.axhline(y=np.mean(subject_corrs), color='red', \n",
    "                   linestyle='--', label=f'Mean: {np.mean(subject_corrs):.3f}')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Feature importance (aggregate)\n",
    "    ax3 = plt.subplot(3, 2, 3)\n",
    "    \n",
    "    # Aggregate feature importance from best model\n",
    "    feature_importance_sum = {}\n",
    "    \n",
    "    for result in model_results:\n",
    "        model = result['results'][best_model]['model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            for i, feat in enumerate(result['top_features']):\n",
    "                if feat not in feature_importance_sum:\n",
    "                    feature_importance_sum[feat] = 0\n",
    "                feature_importance_sum[feat] += model.feature_importances_[i]\n",
    "    \n",
    "    # Sort and plot top features\n",
    "    if feature_importance_sum:\n",
    "        sorted_features = sorted(feature_importance_sum.items(), \n",
    "                               key=lambda x: x[1], reverse=True)[:15]\n",
    "        features = [f[0] for f in sorted_features]\n",
    "        importances = [f[1] for f in sorted_features]\n",
    "        \n",
    "        y_pos = np.arange(len(features))\n",
    "        ax3.barh(y_pos, importances, color='lightcoral')\n",
    "        ax3.set_yticks(y_pos)\n",
    "        ax3.set_yticklabels([f[:30] + '...' if len(f) > 30 else f \n",
    "                            for f in features], fontsize=8)\n",
    "        ax3.set_xlabel('Cumulative Importance')\n",
    "        ax3.set_title(f'Top Features Across All Folds ({best_model})')\n",
    "        ax3.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 4. Example prediction plot\n",
    "    ax4 = plt.subplot(3, 2, 4)\n",
    "    \n",
    "    # Use first split as example\n",
    "    example_result = model_results[0]\n",
    "    y_true = example_result['split_info']['y_test']\n",
    "    y_pred = example_result['results'][best_model]['predictions']\n",
    "    \n",
    "    # Subsample for clarity\n",
    "    max_points = min(300, len(y_true))\n",
    "    ax4.plot(y_true[:max_points], 'k-', label='Actual', linewidth=1.5, alpha=0.8)\n",
    "    ax4.plot(y_pred[:max_points], 'r--', label='Predicted', linewidth=1.5, alpha=0.8)\n",
    "    ax4.set_xlabel('Time Points')\n",
    "    ax4.set_ylabel('PDA (z-score)')\n",
    "    ax4.set_title('Example Prediction (First 300 samples)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cross-subject generalization matrix\n",
    "    ax5 = plt.subplot(3, 2, 5)\n",
    "    \n",
    "    if len(model_results) > 1 and 'test_subject' in model_results[0]['split_info']:\n",
    "        # Create correlation matrix\n",
    "        n_subjects = len(model_results)\n",
    "        corr_matrix = np.zeros((n_subjects, n_subjects))\n",
    "        \n",
    "        subjects = [r['split_info']['test_subject'] for r in model_results]\n",
    "        \n",
    "        for i, test_subj in enumerate(subjects):\n",
    "            for j, train_result in enumerate(model_results):\n",
    "                if i == j:\n",
    "                    corr_matrix[i, j] = train_result['results'][best_model]['test_corr']\n",
    "                else:\n",
    "                    # This would require additional cross-prediction\n",
    "                    corr_matrix[i, j] = np.nan\n",
    "        \n",
    "        im = ax5.imshow(corr_matrix, cmap='coolwarm', vmin=0, vmax=1)\n",
    "        ax5.set_xticks(range(n_subjects))\n",
    "        ax5.set_yticks(range(n_subjects))\n",
    "        ax5.set_xticklabels(subjects, rotation=45, ha='right')\n",
    "        ax5.set_yticklabels(subjects)\n",
    "        ax5.set_xlabel('Test Subject')\n",
    "        ax5.set_ylabel('Test Subject')\n",
    "        ax5.set_title('Generalization Performance')\n",
    "        plt.colorbar(im, ax=ax5, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 6. Summary statistics\n",
    "    ax6 = plt.subplot(3, 2, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    summary_text = f\"\"\"\n",
    "    MULTI-SUBJECT ANALYSIS SUMMARY\n",
    "    \n",
    "    Total Subjects: {len(model_results)}\n",
    "    Features Used: {len(model_results[0]['top_features'])}\n",
    "    \n",
    "    Best Model: {best_model}\n",
    "    Mean Correlation: {mean_corrs[best_model]:.3f}\n",
    "    Std Correlation: {np.std(correlations[best_model]):.3f}\n",
    "    \n",
    "    Performance Range:\n",
    "    Best Subject: {max(correlations[best_model]):.3f}\n",
    "    Worst Subject: {min(correlations[best_model]):.3f}\n",
    "    \n",
    "    All Models Mean±Std:\n",
    "    \"\"\"\n",
    "    \n",
    "    for model in model_names:\n",
    "        summary_text += f\"\\n{model}: {np.mean(correlations[model]):.3f}±{np.std(correlations[model]):.3f}\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes,\n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('Multi-Subject EEG-fMRI Prediction Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlations, best_model\n",
    "\n",
    "\n",
    "# Visualize results\n",
    "correlations, best_model = visualize_multi_subject_results(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ecacf",
   "metadata": {},
   "source": [
    "## Cell 9: Save Final Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccf80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_subject_model(model_results, best_model, common_features, save_path='multi_subject_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save the trained models and necessary information for future predictions.\n",
    "    \"\"\"\n",
    "    # Calculate average performance\n",
    "    all_correlations = []\n",
    "    all_models = []\n",
    "    \n",
    "    for result in model_results:\n",
    "        all_correlations.append(result['results'][best_model]['test_corr'])\n",
    "        all_models.append(result['results'][best_model]['model'])\n",
    "    \n",
    "    # Save comprehensive model data\n",
    "    model_data = {\n",
    "        'model_type': best_model,\n",
    "        'models': all_models,  # All fold models\n",
    "        'feature_names': common_features,\n",
    "        'top_features_per_fold': [r['top_features'] for r in model_results],\n",
    "        'scalers': [r['scaler'] for r in model_results],\n",
    "        'performance': {\n",
    "            'mean_correlation': np.mean(all_correlations),\n",
    "            'std_correlation': np.std(all_correlations),\n",
    "            'per_fold_correlation': all_correlations\n",
    "        },\n",
    "        'processing_params': processing_params,\n",
    "        'subjects_included': selected_subjects,\n",
    "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    joblib.dump(model_data, save_path)\n",
    "    print(f\"\\nModel saved to: {save_path}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    report_path = save_path.replace('.pkl', '_report.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"MULTI-SUBJECT EEG-fMRI PREDICTION MODEL REPORT\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        f.write(f\"Created: {model_data['timestamp']}\\n\")\n",
    "        f.write(f\"Model Type: {best_model}\\n\")\n",
    "        f.write(f\"Subjects: {', '.join(selected_subjects)}\\n\")\n",
    "        f.write(f\"Mean Performance: r = {model_data['performance']['mean_correlation']:.3f} \"\n",
    "                f\"± {model_data['performance']['std_correlation']:.3f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Per-Subject Performance:\\n\")\n",
    "        for i, (subj, corr) in enumerate(zip(selected_subjects, all_correlations)):\n",
    "            f.write(f\"  {subj}: r = {corr:.3f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTop 10 Features (most frequent):\\n\")\n",
    "        feature_counts = {}\n",
    "        for features in model_data['top_features_per_fold']:\n",
    "            for feat in features[:10]:\n",
    "                feature_counts[feat] = feature_counts.get(feat, 0) + 1\n",
    "        \n",
    "        sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for feat, count in sorted_features[:10]:\n",
    "            f.write(f\"  {feat}: {count}/{len(model_results)} folds\\n\")\n",
    "    \n",
    "    print(f\"Report saved to: {report_path}\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "\n",
    "# Save the model\n",
    "saved_model = save_multi_subject_model(model_results, best_model, common_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d49ec9",
   "metadata": {},
   "source": [
    "## Cell 10: Prediction Function for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_subject(edf_path, csv_path, model_data):\n",
    "    \"\"\"\n",
    "    Make predictions for a new subject using the trained multi-subject model.\n",
    "    \"\"\"\n",
    "    print(\"Processing new subject data...\")\n",
    "    \n",
    "    # Load and process EEG\n",
    "    raw = mne.io.read_raw_edf(edf_path, preload=True)\n",
    "    \n",
    "    # Clean EEG\n",
    "    raw_processed, _, _ = automated_channel_cleaning(\n",
    "        raw,\n",
    "        z_score_threshold=model_data['processing_params']['z_score_threshold'],\n",
    "        correlation_threshold=model_data['processing_params']['correlation_threshold'],\n",
    "        powerline_threshold=model_data['processing_params']['powerline_threshold'],\n",
    "        save_report=False\n",
    "    )\n",
    "    \n",
    "    # Load PDA\n",
    "    pda_df = pd.read_csv(csv_path)\n",
    "    pda_signal = (pda_df['cen'] - pda_df['dmn']).values\n",
    "    \n",
    "    # Align PDA\n",
    "    pda_aligned_z, pda_time_aligned = align_pda_to_eeg(\n",
    "        pda_signal, raw_processed,\n",
    "        hrf_delay=model_data['processing_params']['hrf_delay'],\n",
    "        target_fs=model_data['processing_params']['target_fs']\n",
    "    )\n",
    "    \n",
    "    # Extract features (same as training)\n",
    "    # ... (feature extraction code)\n",
    "    \n",
    "    # Make predictions using ensemble of models\n",
    "    predictions = []\n",
    "    \n",
    "    for i, (model, scaler, top_features) in enumerate(zip(\n",
    "        model_data['models'],\n",
    "        model_data['scalers'],\n",
    "        model_data['top_features_per_fold']\n",
    "    )):\n",
    "        # Select and scale features\n",
    "        # X_selected = ... (select top features)\n",
    "        # X_scaled = scaler.transform(X_selected)\n",
    "        \n",
    "        # Predict\n",
    "        # y_pred = model.predict(X_scaled)\n",
    "        # predictions.append(y_pred)\n",
    "        pass\n",
    "    \n",
    "    # Ensemble prediction (average)\n",
    "    # final_prediction = np.mean(predictions, axis=0)\n",
    "    \n",
    "    print(\"Prediction complete!\")\n",
    "    # return final_prediction, pda_aligned_z\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# predictions, actual = predict_new_subject('path/to/new_edf', 'path/to/new_csv', saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328a08c",
   "metadata": {},
   "source": [
    "## Cell 11: Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc89e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MULTI-SUBJECT PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nProcessed {len(selected_subjects)} subjects\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Average correlation: {np.mean(correlations[best_model]):.3f}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Test on completely new subjects\")\n",
    "print(\"2. Optimize hyperparameters with grid search\")\n",
    "print(\"3. Implement real-time prediction pipeline\")\n",
    "print(\"4. Analyze subject-specific differences\")\n",
    "print(\"5. Create channel-reduced versions for practical BCI\")\n",
    "\n",
    "print(\"\\nTo load and use the saved model:\")\n",
    "print(\"model_data = joblib.load('multi_subject_model.pkl')\")\n",
    "print(\"predictions = predict_new_subject(edf_path, csv_path, model_data)\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
